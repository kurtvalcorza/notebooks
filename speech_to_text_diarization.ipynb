{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kurtvalcorza/notebooks/blob/main/speech_to_text_diarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a32f7f92"
      },
      "source": [
        "```markdown\n",
        "# Audio Diarization with OpenAI's `gpt-4o-transcribe-diarize`\n",
        "\n",
        "This notebook demonstrates how to perform audio diarization using OpenAI's `gpt-4o-transcribe-diarize` model. It takes an audio file as input, processes it in chunks, and generates a diarized transcript, identifying different speakers in the audio.\n",
        "\n",
        "## Features\n",
        "\n",
        "*   **Audio Diarization:** Identifies and separates speakers in an audio file.\n",
        "*   **Chunking:** Processes large audio files by splitting them into smaller chunks to meet API limits.\n",
        "*   **Retry Mechanism:** Includes a retry logic for API calls to handle transient errors.\n",
        "*   **Speaker Recognition (Optional):** Supports providing known speaker references for improved diarization.\n",
        "\n",
        "## Setup\n",
        "\n",
        "1.  **OpenAI API Key:**\n",
        "    *   Obtain an OpenAI API key from the [OpenAI platform](https://platform.openai.com/).\n",
        "    *   In Google Colab, go to the \"ðŸ”‘\" (Secrets) icon on the left panel.\n",
        "    *   Add a new secret named `OPENAI_API_KEY` and paste your OpenAI API key as the value.\n",
        "\n",
        "2.  **Install Dependencies:**\n",
        "    The notebook automatically installs the required Python packages (`openai`, `pydub`) and `ffmpeg`.\n",
        "\n",
        "## Usage\n",
        "\n",
        "1.  **Upload Audio File:**\n",
        "    *   Upload your audio file (e.g., `audio.wav`, `meeting.mp3`) to the Colab environment. You can drag and drop it into the file browser on the left panel or use the code cells to upload.\n",
        "\n",
        "2.  **Configure Input File:**\n",
        "    *   Modify the `input_file` variable in the code to point to your uploaded audio file.\n",
        "    *   _Optional:_ If you have a reference audio file for a known speaker, upload it and update `known_speaker_ref_file` and `known_speaker_name` accordingly.\n",
        "\n",
        "3.  **Run All Cells:**\n",
        "    *   Execute all cells in the notebook. The script will process the audio, perform diarization, and save the transcript to a text file (e.g., `audio_diarized_output.txt`).\n",
        "\n",
        "4.  **View Output:**\n",
        "    *   The diarized transcript will be saved in the same directory as a `.txt` file, which you can then download or view directly in Colab.\n",
        "\n",
        "## Output Format\n",
        "\n",
        "The output file will contain lines in the format:\n",
        "\n",
        "```\n",
        "[start_time-end_time] Speaker_Identifier: Transcribed Text\n",
        "```\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "[0.00-2.50] Speaker 1: Hello, how are you today?\n",
        "[2.50-5.10] Speaker 2: I'm doing great, thanks for asking!\n",
        "```\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HBxS_46fc4qI",
        "outputId": "a37841fc-49c1-484f-da85-8c0ccb92402b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Collecting openai\n",
            "  Downloading openai-2.7.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Downloading openai-2.7.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.109.1\n",
            "    Uninstalling openai-1.109.1:\n",
            "      Successfully uninstalled openai-1.109.1\n",
            "Successfully installed openai-2.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U openai pydub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D_Mj0u2uc4qL",
        "outputId": "944d2f82-82e2-4af8-867c-2a8e81adbcc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install ffmpeg -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5QeL-hyc4qM",
        "outputId": "ceeb05ad-cbf2-488b-887c-b02975d678ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading audio: /content/audio.wav\n",
            "Audio duration: 18.36s\n",
            "Transcribing chunk at 0.00s: temp_chunk_0.wav (attempt 1)\n",
            "Diarized transcript saved to /content/audio_diarized_output.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import base64\n",
        "from openai import OpenAI\n",
        "from pydub import AudioSegment\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the API key for the OpenAI package\n",
        "client = OpenAI(\n",
        "    api_key=userdata.get(\"OPENAI_API_KEY\"), # Use Colab's Secrets manager\n",
        ")\n",
        "\n",
        "# Input and output set to your audio (e.g., 'meeting.mp3')\n",
        "input_file = \"/content/audio.wav\"  #@param {type: \"string\"}\n",
        "output_file = os.path.splitext(input_file)[0] + \"_diarized_output.txt\"\n",
        "\n",
        "# Diarization configuration\n",
        "NUM_SPEAKERS = None  # set to an int if you know the count, else leave as None\n",
        "known_speaker_name = \"agent\"\n",
        "known_speaker_ref_file = \"agent.wav\"  # used if present\n",
        "\n",
        "# API chunk limit: keep under 1400 seconds; use a safety buffer\n",
        "# Also control file size by reducing duration and bitrate.\n",
        "MAX_DURATION_MS = 600 * 1000  # 10 minutes per chunk\n",
        "MAX_RETRIES = 5\n",
        "INITIAL_DELAY = 1.0\n",
        "\n",
        "def to_data_url(path: str) -> str:\n",
        "    with open(path, \"rb\") as fh:\n",
        "        return \"data:audio/wav;base64,\" + base64.b64encode(fh.read()).decode(\"utf-8\")\n",
        "\n",
        "def build_extra_body():\n",
        "    eb = {}\n",
        "    if os.path.exists(known_speaker_ref_file):\n",
        "        eb.update({\n",
        "            \"known_speaker_names\": [known_speaker_name],\n",
        "            \"known_speaker_references\": [to_data_url(known_speaker_ref_file)],\n",
        "        })\n",
        "    if isinstance(NUM_SPEAKERS, int) and NUM_SPEAKERS > 0:\n",
        "        eb[\"num_speakers\"] = NUM_SPEAKERS\n",
        "    return eb\n",
        "\n",
        "def process_chunk(chunk_path, chunk_start_ms, extra_body, attempt=0):\n",
        "    if attempt >= MAX_RETRIES:\n",
        "        print(f\"Failed after {MAX_RETRIES} attempts: {chunk_path}\")\n",
        "        return []\n",
        "    try:\n",
        "        with open(chunk_path, \"rb\") as audio_file:\n",
        "            print(f\"Transcribing chunk at {chunk_start_ms/1000:.2f}s: {os.path.basename(chunk_path)} (attempt {attempt+1})\")\n",
        "            resp = client.audio.transcriptions.create(\n",
        "                model=\"gpt-4o-transcribe-diarize\",\n",
        "                file=audio_file,\n",
        "                response_format=\"diarized_json\",\n",
        "                chunking_strategy=\"auto\",\n",
        "                extra_body=extra_body,\n",
        "            )\n",
        "        # Adjust timestamps to absolute\n",
        "        adj = []\n",
        "        offset = chunk_start_ms / 1000.0\n",
        "        for seg in resp.segments:\n",
        "            adj.append({\n",
        "                'speaker': seg.speaker,\n",
        "                'text': seg.text,\n",
        "                'start': seg.start + offset,\n",
        "                'end': seg.end + offset,\n",
        "            })\n",
        "        return adj\n",
        "    except Exception as e:\n",
        "        em = str(e).lower()\n",
        "        print(f\"Error on chunk {chunk_path}: {e}\")\n",
        "        if 'corrupted' in em or 'unsupported' in em or \"param': 'file'\" in em:\n",
        "            # Fallback: re-encode to 16kHz mono WAV and retry once for this attempt level\n",
        "            try:\n",
        "                from pydub import AudioSegment as _AS\n",
        "                wav_path = chunk_path + '.wav'\n",
        "                _AS.from_file(chunk_path).set_channels(1).set_frame_rate(16000).export(wav_path, format='wav')\n",
        "                with open(wav_path, 'rb') as audio_file:\n",
        "                    resp = client.audio.transcriptions.create(\n",
        "                        model=\"gpt-4o-transcribe-diarize\",\n",
        "                        file=audio_file,\n",
        "                        response_format=\"diarized_json\",\n",
        "                        chunking_strategy=\"auto\",\n",
        "                        extra_body=extra_body,\n",
        "                    )\n",
        "                try:\n",
        "                    os.remove(wav_path)\n",
        "                except OSError:\n",
        "                    pass\n",
        "                adj = []\n",
        "                offset = chunk_start_ms / 1000.0\n",
        "                for seg in resp.segments:\n",
        "                    adj.append({\n",
        "                        'speaker': seg.speaker,\n",
        "                        'text': seg.text,\n",
        "                        'start': seg.start + offset,\n",
        "                        'end': seg.end + offset,\n",
        "                    })\n",
        "                return adj\n",
        "            except Exception as _e:\n",
        "                print(f\"Fallback WAV attempt failed: {_e}\")\n",
        "                # continue to generic retry below\n",
        "        if 'rate limit' in em or 'server error' in em:\n",
        "            delay = INITIAL_DELAY * (2 ** attempt)\n",
        "            print(f\"Retrying in {delay:.2f}s...\")\n",
        "            time.sleep(delay)\n",
        "            return process_chunk(chunk_path, chunk_start_ms, extra_body, attempt + 1)\n",
        "        raise\n",
        "\n",
        "# Load audio and split as needed\n",
        "if not os.path.exists(input_file):\n",
        "    raise FileNotFoundError(f\"Input audio not found: {input_file}\")\n",
        "\n",
        "print(f\"Loading audio: {input_file}\")\n",
        "audio = AudioSegment.from_file(input_file)\n",
        "duration_sec = len(audio) / 1000.0\n",
        "print(f\"Audio duration: {duration_sec:.2f}s\")\n",
        "\n",
        "start_ms = 0\n",
        "chunk_idx = 0\n",
        "all_segments = []\n",
        "temp_files = []\n",
        "extra_body = build_extra_body()\n",
        "\n",
        "while start_ms < len(audio):\n",
        "    end_ms = min(start_ms + MAX_DURATION_MS, len(audio))\n",
        "    chunk = audio[start_ms:end_ms]\n",
        "    # Re-encode to mono 16kHz to ensure compatibility and size reduction\n",
        "    chunk = chunk.set_channels(1).set_frame_rate(16000)\n",
        "    # Export WAV (PCM 16kHz mono) to avoid mp3 encoder issues\n",
        "    tmp_name = f\"temp_chunk_{chunk_idx}.wav\"\n",
        "    chunk.export(tmp_name, format=\"wav\")\n",
        "    temp_files.append(tmp_name)\n",
        "\n",
        "    segs = process_chunk(tmp_name, start_ms, extra_body)\n",
        "    for s in segs:\n",
        "        all_segments.append(s)\n",
        "\n",
        "    start_ms = end_ms\n",
        "    chunk_idx += 1\n",
        "\n",
        "# Save diarized output\n",
        "if all_segments:\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for s in all_segments:\n",
        "            f.write(f\"[{s['start']:.2f}-{s['end']:.2f}] {s['speaker']}: {s['text']}\\n\")\n",
        "    print(f\"Diarized transcript saved to {output_file}\")\n",
        "else:\n",
        "    print(\"No segments returned.\")\n",
        "\n",
        "# Cleanup\n",
        "for fp in temp_files:\n",
        "    try:\n",
        "        os.remove(fp)\n",
        "    except OSError as e:\n",
        "        print(f\"Warning: could not delete {fp}: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "whisper",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}