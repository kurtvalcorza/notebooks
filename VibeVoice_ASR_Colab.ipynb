{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kurtvalcorza/notebooks/blob/main/VibeVoice_ASR_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VibeVoice-ASR: Unified Speech-to-Text with Speaker Diarization\n",
    "\n",
    "This notebook demonstrates [Microsoft's VibeVoice-ASR](https://huggingface.co/microsoft/VibeVoice-ASR), a 9B parameter model that provides:\n",
    "\n",
    "- **Who**: Speaker identification/diarization\n",
    "- **When**: Precise timestamps\n",
    "- **What**: Transcribed content\n",
    "- **60-minute single-pass processing** with global context\n",
    "- **Customizable hotwords** for domain-specific accuracy\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- **GPU Memory**: ~18-20GB VRAM recommended\n",
    "- **Colab Runtime**: Use **A100 GPU** (Colab Pro/Pro+) for best results. T4 (16GB) may work with 4-bit quantization.\n",
    "\n",
    "To change runtime: `Runtime` → `Change runtime type` → Select `A100 GPU` or `T4 GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        \"No GPU detected! Please enable GPU runtime:\\n\"\n",
    "        \"Runtime → Change runtime type → Hardware accelerator → GPU\"\n",
    "    )\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"VRAM: {gpu_memory_gb:.1f} GB\")\n",
    "\n",
    "if gpu_memory_gb < 16:\n",
    "    print(\"\\n⚠️  Warning: GPU has less than 16GB VRAM. Model may not fit.\")\n",
    "    print(\"   Consider using Colab Pro with A100 GPU.\")\n",
    "elif gpu_memory_gb < 20:\n",
    "    print(\"\\n⚠️  Note: T4 GPU detected. Using 4-bit quantization for memory efficiency.\")\n",
    "    USE_QUANTIZATION = True\n",
    "else:\n",
    "    print(\"\\n✓ Sufficient VRAM for full-precision inference.\")\n",
    "    USE_QUANTIZATION = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%capture\n# Install core dependencies\n!pip install -U transformers accelerate bitsandbytes\n!pip install soundfile librosa\n!apt-get update -qq && apt-get install -qq ffmpeg\n\n# Install flash-attention for better performance on Ampere+ GPUs (A100, etc.)\n# This may take a few minutes to compile\nimport torch\nif torch.cuda.get_device_capability()[0] >= 8:\n    print(\"Installing flash-attention for Ampere+ GPU...\")\n    !pip install flash-attn --no-build-isolation -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone VibeVoice repository for the processor and utilities\n",
    "!git clone https://github.com/microsoft/VibeVoice.git /content/VibeVoice 2>/dev/null || echo \"Repository already cloned\"\n",
    "!pip install -e /content/VibeVoice[asr] -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mount Google Drive (Optional)\n",
    "\n",
    "Mount your Google Drive to access audio files stored there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load VibeVoice-ASR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport sys\n\n# Add VibeVoice to path\nsys.path.insert(0, '/content/VibeVoice')\n\nfrom vibevoice.modular.modeling_vibevoice_asr import VibeVoiceASRForConditionalGeneration\nfrom vibevoice.processor.vibevoice_asr_processor import VibeVoiceASRProcessor\n\nMODEL_ID = \"microsoft/VibeVoice-ASR\"\n\n# Check if we should use quantization (set in cell 1)\ntry:\n    use_quant = USE_QUANTIZATION\nexcept NameError:\n    use_quant = torch.cuda.get_device_properties(0).total_memory / 1e9 < 20\n\n# Determine attention implementation based on GPU capability\ngpu_capability = torch.cuda.get_device_capability()[0]\nif gpu_capability >= 8:\n    attn_impl = \"flash_attention_2\"\nelse:\n    attn_impl = \"sdpa\"\n\nprint(f\"Loading model: {MODEL_ID}\")\nprint(f\"Using 4-bit quantization: {use_quant}\")\nprint(f\"Attention implementation: {attn_impl}\")\n\n# Load processor (uses Qwen2.5-7B as base language model)\nprocessor = VibeVoiceASRProcessor.from_pretrained(\n    MODEL_ID,\n    language_model_pretrained_name=\"Qwen/Qwen2.5-7B\"\n)\n\n# Load model\nif use_quant:\n    from transformers import BitsAndBytesConfig\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\"\n    )\n    model = VibeVoiceASRForConditionalGeneration.from_pretrained(\n        MODEL_ID,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        attn_implementation=attn_impl\n    )\nelse:\n    model = VibeVoiceASRForConditionalGeneration.from_pretrained(\n        MODEL_ID,\n        dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        attn_implementation=attn_impl\n    )\n\nprint(\"\\n✓ Model loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transcription Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import librosa\nimport numpy as np\nimport re\n\ndef load_audio(audio_path, target_sr=16000):\n    \"\"\"Load audio file and resample to target sample rate.\"\"\"\n    audio, sr = librosa.load(audio_path, sr=target_sr)\n    return audio, sr\n\ndef transcribe(\n    audio_path,\n    hotwords=None,\n    max_new_tokens=None,  # Auto-calculated if None\n    temperature=0.0,\n):\n    \"\"\"\n    Transcribe audio file with speaker diarization and timestamps.\n    \n    Args:\n        audio_path: Path to audio file\n        hotwords: Optional list of domain-specific terms/names to improve accuracy\n        max_new_tokens: Maximum tokens to generate (auto-calculated based on duration if None)\n        temperature: Sampling temperature (0.0 for deterministic)\n    \n    Returns:\n        Dictionary with raw text and parsed segments\n    \"\"\"\n    # Load audio\n    audio_data, sr = load_audio(audio_path)\n    duration = len(audio_data) / sr\n    print(f\"Audio duration: {duration:.1f} seconds\")\n    \n    if duration > 3600:\n        print(\"⚠️  Warning: Audio exceeds 60 minutes. Results may be truncated.\")\n    \n    # Auto-calculate max_new_tokens based on duration if not specified\n    # Estimate: ~3 words/sec * 1.5 tokens/word * 2x buffer for timestamps/speakers\n    if max_new_tokens is None:\n        max_new_tokens = max(256, min(int(duration * 10) + 256, 8192))\n        print(f\"Auto max_new_tokens: {max_new_tokens}\")\n    \n    # Build generation prompt with optional hotwords\n    hotword_str = \", \".join(hotwords) if hotwords else None\n    \n    # Prepare inputs using the VibeVoice processor\n    inputs = processor(\n        audio=audio_data,\n        sampling_rate=sr,\n        return_tensors=\"pt\",\n        padding=True,\n        add_generation_prompt=True,\n    )\n    \n    # Move inputs to model device\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n    \n    # Generate transcription\n    print(\"Generating transcription...\")\n    with torch.inference_mode():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature if temperature > 0 else None,\n            do_sample=temperature > 0,\n            pad_token_id=processor.tokenizer.eos_token_id,\n        )\n    \n    # Decode output - skip the input tokens\n    input_len = inputs['input_ids'].shape[1]\n    generated_ids = outputs[0][input_len:]\n    \n    # Find EOS token and truncate\n    eos_token_id = processor.tokenizer.eos_token_id\n    if eos_token_id in generated_ids:\n        eos_idx = (generated_ids == eos_token_id).nonzero(as_tuple=True)[0]\n        if len(eos_idx) > 0:\n            generated_ids = generated_ids[:eos_idx[0]]\n    \n    transcription = processor.tokenizer.decode(generated_ids, skip_special_tokens=True)\n    \n    # Post-process using VibeVoice's built-in function if available\n    try:\n        from vibevoice.processor.vibevoice_asr_processor import post_process_transcription\n        processed = post_process_transcription(transcription)\n        segments = processed.get(\"segments\", [])\n        raw_text = processed.get(\"text\", transcription)\n    except (ImportError, AttributeError):\n        segments = parse_transcription(transcription)\n        raw_text = transcription\n    \n    return {\n        \"raw_text\": raw_text,\n        \"segments\": segments,\n        \"duration\": duration\n    }\n\ndef parse_transcription(text):\n    \"\"\"\n    Parse VibeVoice-ASR output into structured segments.\n    Expected format: [start-end] Speaker: text or JSON format\n    \"\"\"\n    segments = []\n    \n    # Try JSON format first\n    try:\n        import json\n        # Handle potential incomplete JSON\n        if text.strip().startswith('['):\n            # Try to parse as JSON array\n            data = json.loads(text)\n            for item in data:\n                segments.append({\n                    \"start\": item.get(\"Start\", item.get(\"start\", 0)),\n                    \"end\": item.get(\"End\", item.get(\"end\", 0)),\n                    \"speaker\": f\"Speaker {item.get('Speaker', item.get('speaker', 0))}\",\n                    \"text\": item.get(\"Content\", item.get(\"content\", item.get(\"text\", \"\")))\n                })\n            return segments\n    except (json.JSONDecodeError, TypeError):\n        pass\n    \n    # Fallback to timestamp pattern matching\n    pattern = r'\\[([\\d.]+)[-–]([\\d.]+)\\]\\s*(Speaker\\s*\\d+|[^:]+):\\s*(.+?)(?=\\[[\\d.]|$)'\n    \n    for match in re.finditer(pattern, text, re.DOTALL):\n        segments.append({\n            \"start\": float(match.group(1)),\n            \"end\": float(match.group(2)),\n            \"speaker\": match.group(3).strip(),\n            \"text\": match.group(4).strip()\n        })\n    \n    return segments\n\ndef print_transcript(result, show_timestamps=True):\n    \"\"\"Pretty print transcription result.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"TRANSCRIPTION\")\n    print(\"=\"*60 + \"\\n\")\n    \n    if result[\"segments\"]:\n        for seg in result[\"segments\"]:\n            if show_timestamps:\n                start = seg.get('start', seg.get('start_time', 0))\n                end = seg.get('end', seg.get('end_time', 0))\n                speaker = seg.get('speaker', 'Unknown')\n                text = seg.get('text', '')\n                print(f\"[{start:.2f}-{end:.2f}] {speaker}: {text}\")\n            else:\n                print(f\"{seg.get('speaker', 'Unknown')}: {seg.get('text', '')}\")\n            print()\n    else:\n        # Fallback to raw text if parsing failed\n        print(result[\"raw_text\"])\n    \n    print(\"=\"*60)\n\ndef save_transcript(result, output_path, show_timestamps=True):\n    \"\"\"Save transcription to text file.\"\"\"\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        if result[\"segments\"]:\n            for seg in result[\"segments\"]:\n                start = seg.get('start', seg.get('start_time', 0))\n                end = seg.get('end', seg.get('end_time', 0))\n                speaker = seg.get('speaker', 'Unknown')\n                text = seg.get('text', '')\n                if show_timestamps:\n                    f.write(f\"[{start:.2f}-{end:.2f}] {speaker}: {text}\\n\")\n                else:\n                    f.write(f\"{speaker}: {text}\\n\")\n        else:\n            f.write(result[\"raw_text\"])\n    print(f\"Transcript saved to: {output_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transcribe Audio\n",
    "\n",
    "Set the path to your audio file and optional hotwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "audio_file = \"/content/drive/MyDrive/audio.wav\"  #@param {type: \"string\"}\n",
    "hotwords = \"\"  #@param {type: \"string\"}\n",
    "save_output = True  #@param {type: \"boolean\"}\n",
    "\n",
    "# Parse hotwords\n",
    "hotword_list = [h.strip() for h in hotwords.split(\",\") if h.strip()] if hotwords else None\n",
    "\n",
    "# Run transcription\n",
    "result = transcribe(\n",
    "    audio_path=audio_file,\n",
    "    hotwords=hotword_list\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print_transcript(result)\n",
    "\n",
    "# Save to file\n",
    "if save_output:\n",
    "    import os\n",
    "    output_path = os.path.splitext(audio_file)[0] + \"_vibevoice_transcript.txt\"\n",
    "    save_transcript(result, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Processing (Multiple Files)\n",
    "\n",
    "Process multiple audio files from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "input_dir = \"/content/drive/MyDrive/audio_files\"  #@param {type: \"string\"}\n",
    "output_dir = \"/content/drive/MyDrive/transcripts\"  #@param {type: \"string\"}\n",
    "audio_extensions = [\".wav\", \".mp3\", \".m4a\", \".flac\", \".ogg\"]  # Supported formats\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Find audio files\n",
    "audio_files = []\n",
    "for ext in audio_extensions:\n",
    "    audio_files.extend(Path(input_dir).glob(f\"*{ext}\"))\n",
    "\n",
    "print(f\"Found {len(audio_files)} audio files to process\\n\")\n",
    "\n",
    "# Process each file\n",
    "for i, audio_path in enumerate(audio_files, 1):\n",
    "    print(f\"\\n[{i}/{len(audio_files)}] Processing: {audio_path.name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        result = transcribe(str(audio_path))\n",
    "        \n",
    "        # Save transcript\n",
    "        output_path = os.path.join(output_dir, f\"{audio_path.stem}_transcript.txt\")\n",
    "        save_transcript(result, output_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n\\nCompleted! Transcripts saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Notes\n",
    "\n",
    "### Model Capabilities\n",
    "- Processes up to 60 minutes of audio in a single pass\n",
    "- Maintains consistent speaker tracking across long recordings\n",
    "- Supports English and Chinese\n",
    "\n",
    "### Hotwords\n",
    "Use hotwords to improve accuracy for:\n",
    "- Names of people, products, or companies\n",
    "- Technical terminology\n",
    "- Domain-specific vocabulary\n",
    "\n",
    "### Resources\n",
    "- [VibeVoice GitHub](https://github.com/microsoft/VibeVoice)\n",
    "- [Model on Hugging Face](https://huggingface.co/microsoft/VibeVoice-ASR)\n",
    "- [Live Demo](https://aka.ms/vibevoice-asr)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}