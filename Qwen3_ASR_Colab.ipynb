{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Qwen3-ASR-1.7B Simple Implementation\n",
                "\n",
                "This notebook demonstrates how to use the Qwen3-ASR-1.7B model for Automatic Speech Recognition (ASR). Qwen3-ASR is a powerful and lightweight model from the Qwen team.\n",
                "\n",
                "Developed by [Alibaba Qwen Team](https://huggingface.co/Qwen)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment\n",
                "\n",
                "First, we need to install the `qwen-asr` package. We also recommend installing `flash-attn` for faster inference if you have a compatible GPU."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install the core package\n",
                "!pip install -U qwen-asr\n",
                "\n",
                "# Optional: install FlashAttention 2 for faster inference (requires GPU)\n",
                "# !pip install -U flash-attn --no-build-isolation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load the Model\n",
                "\n",
                "We will load the 1.7B model using the Transformers backend. This is the most straightforward way to run it in a notebook environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from qwen_asr import Qwen3ASRModel\n",
                "\n",
                "# Check if GPU is available\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "model = Qwen3ASRModel.from_pretrained(\n",
                "    \"Qwen/Qwen3-ASR-1.7B\",\n",
                "    dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
                "    device_map=device,\n",
                "    max_inference_batch_size=32,\n",
                "    max_new_tokens=256\n",
                ")\n",
                "\n",
                "print(\"Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Transcribe Audio (Example)\n",
                "\n",
                "Let's test the model with a sample audio file from the Qwen team's repository."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_audio_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-ASR-Repo/asr_en.wav\"\n",
                "\n",
                "results = model.transcribe(\n",
                "    audio=sample_audio_url,\n",
                "    language=None # Automatically detect language\n",
                ")\n",
                "\n",
                "print(f\"Detected Language: {results[0].language}\")\n",
                "print(f\"Transcription: {results[0].text}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Transcribe Your Own Audio from Google Drive\n",
                "\n",
                "Mount your Google Drive and provide the path to your audio file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Define the path to your audio file in Google Drive\n",
                "audio_path = '/content/drive/MyDrive/Voice 251201_131937.m4a' #@param {type: \"string\"}\n",
                "\n",
                "if os.path.exists(audio_path):\n",
                "    print(f'Transcribing file: {audio_path}')\n",
                "    results = model.transcribe(\n",
                "        audio=audio_path,\n",
                "        language=None\n",
                "    )\n",
                "    print(f\"\\nDetected Language: {results[0].language}\")\n",
                "    print(f\"Transcription Result:\\n{results[0].text}\")\n",
                "else:\n",
                "    print(f\"File not found: {audio_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}