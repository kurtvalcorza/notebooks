{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFtEeY13V5le"
      },
      "source": [
        "# Setup\n",
        "- Upload audio mp3 file/s to Google Drive\n",
        "- Change runtime type Hardware accelerator to T4 GPU\n",
        "- Install Whisper and mount Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNmYH7tsGWkU"
      },
      "source": [
        "# whisper-large-v2\n",
        "- [https://github.com/openai/whisper](https://github.com/openai/whisper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-oYXJ64VDD0"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d6CY0USYMq8"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpoREdPV2bkD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the input directory containing audio files\n",
        "input_dir = '/content/drive/MyDrive/'  #@param {type: \"string\"}\n",
        "\n",
        "# Iterate over audio files in the input directory\n",
        "for audio_file in os.listdir(input_dir):\n",
        "    if audio_file.endswith('.mp3'):  # Adjust the file extension as needed\n",
        "        audio_path = os.path.join(input_dir, audio_file)\n",
        "\n",
        "        # Transcribe the audio file\n",
        "        !whisper \"{audio_path}\" --model large-v3 --output_dir \"{input_dir}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTKIj0FQGmBV"
      },
      "source": [
        "# whisper-large-v3\n",
        "- https://huggingface.co/openai/whisper-large-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqKDaX6iGxdK",
        "outputId": "a28093bd-1b95-48ca-bf57-97a5b3b1953b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: datasets[audio] in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.70.15)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.13.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.11.0)\n",
            "Requirement already satisfied: soxr>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.5.0.post1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->datasets[audio]) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.22)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (1.8.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (1.1.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->datasets[audio]) (4.3.8)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->datasets[audio]) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[audio]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[audio]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[audio]) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets[audio]) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install --upgrade pip\n",
        "!pip install --upgrade transformers datasets[audio] accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZJfWqMjG93z",
        "outputId": "1ba48636-ed34-4998-a6b0-a5910ef27047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "08a8868d60a8431d857eae23eb74391a",
            "316fcf94cc18441cba6f7bf1cf5875af",
            "cabb50fc61f6455982d653a675573a43",
            "fafabedf0c5349d5a79b498ada2f438a",
            "f50c9d733a874e1882e76f5040586901",
            "59057142c7484021862763c837a83df0",
            "8b9c2ffc7d374a88824e8ccbd7cf6c60",
            "c67ea78f05754723a36db9d5bfc6d847",
            "160ddf2c05c343919dfc193e74ed1be6",
            "ee3778a77d914c558ae2fe11e9dd9e50",
            "c5b1612bd0cd4a0f8af03a31d9991c15",
            "30345451f2eb4039b690c52bd65701e3",
            "bdc7e28550b443dab917634690a9cf93",
            "24deefad2ac4452b8e644bc876141310",
            "668e8c1c3b5f4eb69268bf97df5823be",
            "9ee5a51e28d949f3b077af5f9e9ed963",
            "7c9391398ab34a0a8ac1ee86ab50f2af",
            "e933ce06b5f343b0845fcef8c5dbd394",
            "e622d6c7395f4038b6c27072929402c4",
            "b2779945066e4348a48703d443b29894",
            "817b99d3bc3f44b1ab406775933088fd",
            "8a5f4846287046a6a2fa97d625d2ba0f",
            "e26086736e2149faa30264fdc47466e7",
            "f8507a542f484955817bf60c9d4e78a7",
            "d113289f24d74dc8891835d0681fcd64",
            "4291cf4cd8674aff933bea5e5e18711a",
            "d127825763d946a4bf35dc1d05befa9c",
            "93bcf0ebd8f54c3894cc74cc627e980f",
            "fb874ab6059a43fe9c71c54419a490e0",
            "8c507718062f431e80166cff199f020c",
            "0a49c00ec90d4c62bb2a906224dd94e9",
            "a91197a82e4f4305a1245fe984126ccc",
            "10b071612b784e26b02a2a66a08a748a",
            "eea893e32ec44ebcb3e60c8ec70fc655",
            "a7acc423177344c29255a76bbd159926",
            "79f540767d5742cb850bbbe12486592d",
            "b4763e33f00c4410934c8a25a9cad0be",
            "91d4e029699e4afca30735baa5aef536",
            "478c8f31831c4eca8c1ed12782dfe9fa",
            "2e3caa93a0294b16b394907481353e04",
            "ed02b7d8ceeb47e3a07e50dab0437c30",
            "92579b31e232496a8144e3cf54684b17",
            "dc326222c6fb4aa6999cfe3157b56bcc",
            "d6d12aa086324481b045808a2e8bfe7e",
            "09fa0c2ca867419fa27ad0f328a971db",
            "5008ac00c9624a6f83018c54446eb0fa",
            "e1af3efdb0524df69999b4b73e9c9191",
            "f0ceac8662e645e18f1af107f14ec391",
            "3b8631a071644d238093f16fac832687",
            "696d2e73123945beb01dd327a2630370",
            "3a02f280684846e2a680eefa37689324",
            "030f700ed92243de9dae7ea7a265985b",
            "a7c25b6c57bd4fa688f5124283564e5f",
            "79701dd790034c8c8b2a4b12d50c0c64",
            "d494ae756a0d4abab7e91db7deda15e2",
            "724c362bdd5e4470ba4f520222a32857",
            "3e37c44e9f1c45928c0945b8208f53c0",
            "58de999e8ced4d2bb0cd62493a51a0fc",
            "fa7c3a2a7a584403b42be774d32cf9e7",
            "8f7a229642474b7f9fa52ac56c23f18b",
            "c18135cd2c7e411c89bd7a275d722481",
            "73eee035866048719e3c7a19d72f8ce8",
            "e7a39542c8e14d8694b2ff13d6b91fa7",
            "2d39b1d70e234f1cb2d5ade7f06894dd",
            "c1c28c1edf824c408a5b39c8490d79ed",
            "f3791f0e8b184963ae85c4e3c7b11a07",
            "6f1b35db6f2e4876be68e0123b1dbb40",
            "72e0c1c626854ccf862e3768d51fe24c",
            "0f37d5a9ed39450686ee81514bea7e79",
            "51081cfec6fc4bc1a95643ab344a6ed1",
            "6d2c2985d7254c0b8c938c2b63cc41c1",
            "ef9b7524a5e940a4aeaeead56cee73be",
            "c6528d0150374a3589941b2d43897d28",
            "3d9f0477250f4859bbe47995c7b31e8d",
            "6230bbe983b34cab9ff23a5aac661447",
            "a53554ffb7c2432a9c48ed4cf69baa26",
            "4d77691baaa745789a513174755e64e5",
            "6aec4510260d418b970ed7a3a49c91d4",
            "1c3183931e734894a661c30f81b119fb",
            "8dfbd4830b5048afb77906e1d495a1d6",
            "3f40ff17c00a4f1bb6baae4d836609bc",
            "7a6cc240c25849c29d78a8e4d30505f2",
            "40d58c5ecc5d4e6a893f7e8c35a4ddbb",
            "8fac2a76034e423c9fc8f92c0036390c",
            "0b3d9577aa04444babc11fab923f6785",
            "26672faa0ff84f23b5c1ec5ba9431761",
            "db35198aacaf41a9afe8168bf7c42e54",
            "24316f423e50444782dfded6e198b93f",
            "a0ebfccaaf62441a9df7a3af42820115",
            "e7cc3b68821648a6bb8e2d5c68e9eab2",
            "5694d791d7434157a4c7221aedfe40e6",
            "019ee4b3a9c64f8fab5f4d7e0ac46297",
            "77d2a25db3cc4d43af9390165e6a3133",
            "ad1bdc86e0aa4c71a1288137b15613c6",
            "9981a366c1ac46a282ba7fd5431748fb",
            "f93b430ba3d542c490c5a9d1a307fd08",
            "9e5c7cfd70db41d29805aa07509e7177",
            "2fac794af5b54e988f9fad9e5e468ebe",
            "feeccf4e643d4f1983e5e41dc6c1cbf3",
            "287e24831a6f497ebaa2d41cc5c69045",
            "df05d5baf9ff43f88d7ec3ba3b1e830a",
            "2a774dfd78754b3c915291fa8d1be864",
            "d7b6c0628314415e830eca4e6d56007c",
            "2a5963bf1e474172bcec724f02294c4b",
            "46c344ced7654c5797eff636ae8aadd3",
            "ef5df9be1dcb421298db8cfcb68a7b73",
            "ea507e5561a74da0a91d51a001bf3533",
            "b0a8e0d036dc4140bcd11f7a9c4af34f",
            "91a1f8e218834ed5ab42d5bc74baf59f",
            "8dc9fc4480d149bdab2bbb125352b231",
            "fde545cea1564e4fad941f43e504c407",
            "261bc6782d324374a9669a76d0aeb1f8",
            "a154181fb5e741f0b20c303ee40396af",
            "a18e7d2b9fb64a9bad10958586a05145",
            "f2d575612e3248ec9c2d2a460d7d5e9e",
            "249c500199c84467bf537c92906a5b63",
            "64e63c760f884f56ad77c9aa0f5b47d0",
            "0a2c81bb533141e7ba54f97329c44148",
            "770e249c72844ef69b05de157cab65e0",
            "fbd6e67551a344aca2d1f7851039a67b",
            "cfb559e7fa2342ad841d5381945c7aef"
          ]
        },
        "id": "2xs6xd29mcI1",
        "outputId": "99c479b2-ec31-4c76-a324-ac3c8b8d7170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08a8868d60a8431d857eae23eb74391a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30345451f2eb4039b690c52bd65701e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e26086736e2149faa30264fdc47466e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eea893e32ec44ebcb3e60c8ec70fc655",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09fa0c2ca867419fa27ad0f328a971db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "724c362bdd5e4470ba4f520222a32857",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f1b35db6f2e4876be68e0123b1dbb40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6aec4510260d418b970ed7a3a49c91d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0ebfccaaf62441a9df7a3af42820115",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "287e24831a6f497ebaa2d41cc5c69045",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fde545cea1564e4fad941f43e504c407",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "Transcribing audio files:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🗂️ Transcribing: /content/drive/MyDrive/AI Governance.wav\n",
            "📏 Total duration: 4455.84 sec → 149 chunks of 30 sec\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   0%|          | 0/149 [00:00<?, ?it/s]\u001b[ADue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "\n",
            "Chunks:   1%|          | 1/149 [00:05<14:10,  5.75s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[000.00 --> 030.00] Have you ever stopped to consider just how much of our everyday lives, especially, you know, when we interact with public services, is now quietly shaped by algorithms? It's really kind of remarkable how artificial intelligence has become this invisible hand guiding things. Yeah. Everything from, well, how your mail gets sorted to potentially much bigger things, right? Yeah. Like health care resource allocation or even law enforcement priorities. Exactly. It's not just about the latest app on your phone.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   1%|▏         | 2/149 [00:09<10:54,  4.45s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[030.00 --> 060.00] This stuff is deeply embedded in the public sphere now. It's powering what we might call AI-powered economies. It's true. We're really seeing this exponential growth of AI's presence in government and public services. Right. And, well, with that growth comes this really urgent critical need to think carefully about how these incredibly powerful tools are actually developed and deployed. deployed. Which is why today we're taking a deep dive into the government perspective on emergency\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   2%|▏         | 3/149 [00:13<10:48,  4.44s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[060.00 --> 090.00] emerging technology R&D, AI-powered economies, how governments are shaping the future. We're really digging into how public bodies are grappling with this new reality. And our main source for this, our foundation, is a really crucial guide from the Alan Turing Institute. It's called Understanding Artificial Intelligence Ethics and Safety, a guide for the responsible design and implementation of AI systems in the public sector. Dr. David Leslie authored it. Yeah, and this guide, it isn't just abstract.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   3%|▎         | 4/149 [00:17<10:02,  4.15s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[090.00 --> 120.00] theory, it's really designed for policymakers, for the people leading public service delivery. It gives them a concrete roadmap. The goal is basically to ensure that AI systems in the public sector are developed and deployed ethically, safely, and responsibly, right from the very start, you know, not just tacked on later. Right, not as an afterthought. So our mission today is to unpack exactly how governments are trying to balance the immense promise of AI, all those incredible benefits we keep hearing about with.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   3%|▎         | 5/149 [00:21<10:02,  4.18s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[120.00 --> 150.00] these critical considerations, ethics, safety, societal impact. It's a huge balancing act. Think of this as your shortcut to understanding this complex landscape of responsible AI governance. We'll highlight the challenges, sure, but also the practical frameworks being developed to tackle them head on. So let's start unpacking. Sounds good. Let's begin by sort of laying the ethical groundwork. And the Alan Turing Institute, our source here, is really central to this. Their public policy program set up back in May.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   4%|▍         | 6/149 [00:28<11:53,  4.99s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[150.00 --> 180.00] 2018, it wasn't just about playing with data, was it? No, not at all. Its core aim is specifically to help governments innovate with these data-intensive technologies for public benefit. That purpose is key. Purpose-driven innovation. I like that. And what really defines their approach is this core belief. Governments can only truly benefit from AI if ethics and safety are treated as a first priority. It has to be foundational. Foundational. Got it. What's also quite interesting about this guide is that they call it a living\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   5%|▍         | 7/149 [00:32<11:09,  4.72s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[180.00 --> 210.00] document. A living document. Yeah. It's designed to evolve, to improve with continuous input and feedback from the people using it. And importantly, the stakeholders affected by it. So it's not like some static rule book. It's meant to be dynamic, responsive. That makes perfect sense. I mean, the tech itself is evolving so fast, the guidance needs to keep up. Exactly. So let's pin this down. What exactly is AI ethics and why is it so, so crucial, especially when we're talking about public sector applications. Right. Well, AI...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   5%|▌         | 8/149 [00:36<10:23,  4.42s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[210.00 --> 240.00] AI ethics is essentially a set of values, principles, and techniques. And these are derived from widely accepted standards of right and wrong. They're meant to guide moral conduct throughout the entire AI lifecycle development use everything. So like a moral compass for the technology. Pretty much. Its main purpose is to motivate morally acceptable practices and also to prescribe the duties, the obligations needed to make sure AI applications are ethical, fair, and safe. It's about, you know, instilling a kind of conscience into the.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   6%|▌         | 9/149 [00:40<10:16,  4.40s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[240.00 --> 270.00] technology by guiding its creators. We hear so much about AI's potential, don't we? The guide calls it a remarkable time of human promise. Big data, cloud computing, advanced machine learning, they're driving these dramatic improvements everywhere. Healthcare, education, transport. Absolutely. And the guide points out that these bounties are, in fact, likely just the start. Because AI and machine learning systems, they inherently get better with more data, more computers.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   7%|▋         | 10/149 [00:44<10:06,  4.36s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[270.00 --> 300.00] power. Right. So they're only going to become more effective. The guide actually calls AI a gatekeeper technology for advancing vital public interests and sustainable human development. Wow. Gatekeeper technology. That's a really strong phrase. It implies a huge amount of power and responsibility. It does. It suggests an almost unavoidable role for AI in our future. But as with any tech that evolves this quickly, there's always a flip side, isn't there? The guide doesn't exactly paint a purely rosy picture. It acknowledges risks. Oh, definitely not. It's very clear on it.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   7%|▋         | 11/149 [00:48<09:27,  4.11s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[300.00 --> 330.00] It states that like any new tech with a steep learning curve, unanticipated and harmful impacts will inevitably occur. AI is no exception here. So it's not if, but when. Exactly. It's not a possibility. The guide frames it as a certainty. And this inevitability is precisely why making ethics and safety a first priority at every single stage of an AI project is just so incredibly critical. You have to anticipate harm. So it's not just about stopping.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   8%|▊         | 12/149 [00:53<09:50,  4.31s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[330.00 --> 360.00] people using AI maliciously, which is one thing, but about foreseeing those unintended consequences. That sounds much harder. It is much harder. And who's responsible for that foresight? Is it just down to the data scientists writing the code? No, absolutely not. The guide really emphasizes that responsible AI needs a truly collaborative effort. You need multidisciplinary teams. Right. We're talking data scientists, product managers, engineers, crucially domain experts who understand the context, delivery managers, they all need to work together.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   9%|▊         | 13/149 [00:56<09:25,  4.16s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[360.00 --> 390.00] The goal is to align the actual development of the AI with these ethical values. Yeah. It's got to be team sport, you know, shared responsibility. That makes sense. Collaboration is key. Now let's dig into what those unanticipated and harmful impacts might actually look like. The guide outlines six pretty significant forms of harm from misuse, abuse, poor design, or just negative unintended consequences. We should probably walk through these. Yeah, definitely. The first one they highlight is bias and discrimination.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:   9%|▉         | 14/149 [01:00<09:03,  4.03s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[390.00 --> 420.00] This is a big one. I bet. Because data-driven technologies, they learn from existing societal structures, right? So as a result, they can end up reproducing, reinforcing, and sometimes even amplifying existing patterns of marginalization, inequality, discrimination. If your training data reflects historical injustices, well, the algorithm is going to learn those injustices. So the biases that are already out there in the world just get sort of baked right into the algorithms, like a feedback loop of it.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  10%|█         | 15/149 [01:05<09:34,  4.29s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[420.00 --> 450.00] inequality. Exactly that. And it's not just the data. Designers own preconceptions, their unconscious biases. They can also be inadvertently replicated through the choices they make about, say, what features to use or what metrics optimize for. And if the data samples you use aren't representative of the population you're trying to make inferences about, well, you end up with flawed inputs. And that leads to a very real possibility of biased discriminatory outcomes right from the get-go. It's a huge, huge challenge. Okay, bias and discrimination.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  11%|█         | 16/149 [01:09<09:13,  4.16s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[450.00 --> 480.00] What's the next potential harm? The second is denial of individual autonomy, recourse, and rights. This is interesting. When AI systems start automating cognitive functions that humans used to do. Like making decisions. Yeah, exactly. Making judgments or decisions. It can create what the guide calls an accountability gap. Because the process is so complex, distributed across different people and systems, it becomes incredibly difficult to pinpoint who is actually responsible if something goes wrong. If there's an injury,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  11%|█▏        | 17/149 [01:13<08:52,  4.03s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[480.00 --> 510.00] or a negative consequence. So if an algorithm makes a bad call that harms someone, it's hard to hold anyone accountable. That sounds like a massive problem for due process and individual rights. It really is. This accountability gap can fundamentally harm a person's autonomy, and it can potentially violate their rights because the system acts, but the human who behind that action gets obscured, lost in the complexity. And related to that, I guess, is transparency. We hear that word all the time in AI discussions.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  12%|█▏        | 18/149 [01:17<09:13,  4.22s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[510.00 --> 540.00] do. And that brings us neatly to the third harm, non-transparent, unexplainable or unjustifiable outcomes. Many advanced machine learning models, people often call them black box models. Right. The black box. They operate on these really high dimensional correlations that are just beyond human ability to interpret easily. So the reasoning, the rationale behind the outcome, the algorithm spits out, remains opaque, hidden, especially to the individuals affected by it. So it gives you an answer, maybe predict something, but you have absolutely no\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  13%|█▎        | 19/149 [01:25<11:06,  5.13s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[540.00 --> 570.00] idea why it reached that conclusion. Exactly. Now, maybe in some low stakes uses like, you know, recommending a movie, maybe that opacity is okay, but it becomes deeply problematic in applications where the data being processed could harbor discrimination, bias, or unfairness. You need to know why a decision was made, especially if it seriously impacts your life, your access to public services, or your fundamental rights. Absolutely. And of course, privacy. That's Always a huge concern with data-driven tech. Always. That's the...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  13%|█▎        | 20/149 [01:29<10:35,  4.92s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[570.00 --> 600.00] fourth arm, invasions of privacy. And the threats here can arise both from the design and development process itself. Like how? Well, think about using personal data without proper consent, or maybe risky handling practices that accidentally expose sensitive information. But threats also come from the deployment phase, how the AI is used. Systems might target, profile, or maybe even nudge individuals without their knowledge or consent. nudging without consent. That sounds particularly, well, manipulative, almost.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  14%|█▍        | 21/149 [01:33<10:03,  4.71s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[600.00 --> 630.00] like psychological steering. It can feel that way. And it can infringe upon a person's ability to lead a private life, to manage how technology influences their development. It really undermines their basic right to pursue their own goals free from, you know, unchosen influence. That connects to a broader societal impact too, doesn't it? How we interact with each other. It does, yeah. The fifth harm they identify is isolation and disintegration of social connection. AI has this amazing capacity for curating.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  15%|█▍        | 22/149 [01:37<09:23,  4.43s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[630.00 --> 660.00] experiences. Hyper personalization. Which sounds good for consumers maybe. It can be yeah. Improve consumer life. Yeah. But it also carries risks like excessive automation might reduce the need for actual human to human interaction in some areas. And hyper personalization while convenient can inadvertently polarize social relationships. If you're only ever shown things that align with your existing views it limits your exposure to diverse world views. It pushes us into echo chambers. So instead of bringing us together, it could access.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  15%|█▌        | 23/149 [01:41<08:47,  4.19s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[660.00 --> 690.00] push us further apart. Filter bubbles on a societal scale. Exactly. The guide really emphasizes how important it is to preserve trust, empathy, mutual understanding, the things that make societies cohesive, especially as AI becomes more widespread. These are fundamental for a healthy society, and AI should ideally support, not undermine them. Okay. And the last one, what about just the quality of what the AI actually produces? That's the sixth harm, unreliable, unsafe, or poor quality outcomes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  16%|█▌        | 24/149 [01:45<09:06,  4.37s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[690.00 --> 720.00] Simply put, irresponsible data management, negligent design, maybe questionable deployment practices, they can all lead to AI systems just not working well, producing unreliable, unsafe, or simply poor quality outcomes. And in the public sector, that's not just annoying. It could be really damaging. Absolutely. It can mean direct damage to the well-being of individuals or the public welfare more broadly. And beyond that immediate harm, what are the knock-on effects? Well, beyond that, it can profoundly undermine public trust in the responsible use of AI overall.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  17%|█▋        | 25/149 [01:50<08:57,  4.33s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[720.00 --> 750.00] If people see failures, they lose faith. And it can lead to harmful inefficiencies, too, wasting limited public resources on AI tech that's ineffective or even detrimental. So, yeah, these six harms really underscore just how critical it is to get AI ethics right, especially in government. That is a comprehensive list and, frankly, a bit sobering. It really highlights why governments can't just, you know, jump on the AI bandwagon blindly. They need a solid framework. Exactly. So let's talk about that framework. The guide introduces the idea.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  17%|█▋        | 26/149 [01:54<08:44,  4.26s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[750.00 --> 780.00] of an ethical platform for responsible AI delivery. What's that about? Right. So this ethical platform concept, it's envisioned as a kind of governance architecture. It's meant to unite two things, an inbuilt culture of responsible innovation and the actual practices that bring ethical, fair, and safe AI to life. It serves a dual purpose. Okay. A dual purpose. What are they? Well, first, it gives project teams a solid process-based footing. Think of it as an ethical foundation they can build on to design and implement AI systems ethically.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  18%|█▊        | 27/149 [01:58<08:50,  4.35s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[780.00 --> 810.00] equitably, safely. So the how-to. Right. And second, it aims to help facilitate a culture of responsible AI innovation. It's about fostering a shared commitment among all the teams involved, a commitment to developing AI for the public good. It's trying to cultivate that shared mindset, not just, you know, ticking compliance boxes. Building a foundation and fostering a culture, that sounds like a powerful combination. What are the main goals for project teams working within this kind of framework.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  19%|█▉        | 28/149 [02:02<08:37,  4.28s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[810.00 --> 840.00] out four primary goals. Teams need to make sure their AI project is first, ethically permissible. That means really thinking through its impact on stakeholders, on communities. Second, it needs to be fair and non-discriminatory. That means actively working to mitigate biases at every stage. Right, which we just discussed. Third, it has to be worthy of public trust. That comes down to ensuring its safety, accuracy, reliability, security, robustness, all those technical aspects. Trust is huge. Absolutely. And fourth, it needs to be justifiable.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  19%|█▉        | 29/149 [02:06<08:16,  4.14s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[840.00 --> 870.00] means prioritizing transparency of the process itself and the interpretability of the outcomes. Can you explain it? Can you justify it? Ethically permissible, fair, trustworthy, justifiable. Got it. But, you know, not all AI projects are the same, are they? Like a spam detector versus a cancer diagnosis tool. The stakes are vastly different. Does the platform account for that? It does. Yeah. That's a really crucial preliminary consideration the guide brings up. Proportionality. Proportionality.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  20%|██        | 30/149 [02:11<08:33,  4.32s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[870.00 --> 900.00] clearly states, you know, a machine learning algorithm just detecting spam emails is going to present fewer ethical challenges than one trying to detect cancer and blood samples. Obvious. So projects that aren't safety critical, that don't directly impact people's lives in major ways and don't process highly sensitive data, they'll require less intensive, proactive ethical stewardship. You tailor the oversight to the level of risk. So more risk, more scrutiny, less risk, Maybe a later touch. Exactly. However, and this is really important.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  21%|██        | 31/149 [02:15<08:06,  4.13s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[900.00 --> 930.00] However, the guide stresses that all AI projects have social and ethical impacts, even if it's just by diverting public resources away from something else or affecting public perception of AI. Ah, OK. So even the low-stakes ones aren't entirely impact-free. Right. So ethical considerations and this kind of principles-based policy thinking should play a salient role in every prospective AI project, even if the depth of the ethical review varies depending on the context and potential impact. That's a critical nuance.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  21%|██▏       | 32/149 [02:19<08:10,  4.19s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[930.00 --> 960.00] project gets a completely free pass ethically. So how does this ethical platform actually get built? What are its foundational components, its building blocks? The guide identifies three main building blocks for what it calls a responsible AI project delivery ecosystem. And they're designed to work together, forming this continuous cycle of reflect, act, and justify. Reflect, act, justify. Okay, what are the three blocks? They are the SUM values, the fast track principles, and the process-based governance PBG framework. FASCM, FASTAFI, PBG. Got it. So the SMV.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  22%|██▏       | 33/149 [02:24<08:28,  4.38s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[960.00 --> 990.00] values are for reflecting, thinking about the ethical purposes, the objectives. The fast-track principles are for acting, making sure every step taken actually produces ethical, fair, safe AI innovation. And the PBG framework is for justifying setting up transparent processes that ensure end-to-end transparency and accountability. Okay, let's start with those SUM values. They sound like the moral compass you mentioned earlier. Where do they come from? What are they based on? Yeah, they provide what the guy calls a moral vocabulary.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  23%|██▎       | 34/149 [02:28<08:09,  4.26s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[990.00 --> 1020.00] And it's drawn primarily from two pretty well-established traditions. First, bioethics, you know, the ethics around medicine and biology, which focuses heavily on protecting individuals, their well-being, equitable treatment. Right, like do no harm. Exactly. And second, human rights discourse, which is anchored in these universal principles like human dignity, civil rights, political rights, social rights, community participation. So it's a blend, really, of medical ethics and universal human rights principles. Interesting blend. So what are the four specific esiums?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  23%|██▎       | 35/149 [02:31<07:44,  4.08s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1020.00 --> 1050.00] values? Can you break them down? Sure. They are respect, connect, care, and protect. They really lay the moral foundation. First, respect the dignity of individual persons. This means things like ensuring people can make free and informed decisions about AI affecting them, safeguarding their autonomy, their right to be heard, and supporting their ability to contribute to the community and pursue their talents and passions. It's fundamentally about recognizing the intrinsic worth of every single individual when AI is involved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  24%|██▍       | 36/149 [02:36<08:04,  4.29s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1050.00 --> 1080.00] So AI should empower individuals, support their flourishing and agency, not diminish it. Makes sense. What's next? Second, connect with each other sincerely, openly and inclusively. This value is about safeguarding the integrity of, well, interpersonal dialogue, meaningful human connection, social cohesion. The social fabric. Exactly. It prioritizes diversity, participation, inclusion at all points in the AI lifecycle. It really encourages making sure all voices are heard. And it pushes for using AI pros socially to enable self-awareness.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  25%|██▍       | 37/149 [02:40<07:45,  4.15s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1080.00 --> 1110.00] Solidarity, reinforced trust, empathy, mutual understanding among people. So AI shouldn't just optimize things for individuals in isolation. It should actively foster healthy social bonds. That's a powerful point and maybe one that gets overlooked sometimes. I think it often does. Third is care for the well-being of each and all. This is about designing and deploying AI systems specifically to foster and cultivate the welfare of all stakeholders involved. It's a direct call to do no harm with these technologies. The\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  26%|██▌       | 38/149 [02:44<07:31,  4.07s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1110.00 --> 1140.00] bioethics influence again? Right. Minimize risks of misuse or abuse. Prioritize people's safety, their mental and physical integrity. It really demands foresight, thinking ahead about the consequences when considering new tech possibilities or deploying AI apps. Do no harm and think ahead. Got it. Finally, protect. Fourth, protect the priorities of social values, justice, and the public interest. This value dictates that we must treat all individuals equally, protect social equity. It means using digital tech to support.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  26%|██▌       | 39/149 [02:48<07:41,  4.20s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1140.00 --> 1170.00] fair and equal treatment under the law. Justice and fairness. Yes. It emphasizes prioritizing social welfare, the public interest. It encourages using AI to empower and advance the interest of as many individuals as possible. And crucially, it demands thinking big picture about the wider impacts of AI, not just now, but for future generations and even for the environment, the biosphere. Wow, that's a truly comprehensive ethical view. So these SUM values, They're meant to be like a constant reference point.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  27%|██▋       | 40/149 [02:53<07:34,  4.17s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1170.00 --> 1200.00] throughout a project. That's exactly the idea. They should orient the deliberation about whether a potential AI project is even ethically permissible in the first place. And they should guide how teams consider the impact across the entire innovation lifecycle. The guide encourages project teams to actively discuss how to weigh these values, especially if the specifics of a use case create tension between them because sometimes they might conflict. Right. Tradeoffs are inevitable sometimes. Okay. So SCM values are the big picture moral compass. Now let's move to the next building block.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  28%|██▊       | 41/149 [02:57<07:25,  4.13s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1200.00 --> 1230.00] the fast track principles. You said these are more actionable, more tailored to the AI development process itself. Why was this extra layer needed? Why not just stick with the SUM values? That's a really good question. It gets right to the heart of why AI ethics has become its own field, really. See, when humans perform intelligent actions, make judgments, we hold them responsible, right, for the accuracy, the soundness of their judgments. Yeah, of course. But AI systems, they aren't human. They're machines.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  28%|██▊       | 42/149 [03:01<07:37,  4.27s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1230.00 --> 1260.00] They shift a lot of cognitive functions over to algorithmic processes. And these processes, as inert machinery, they aren't morally accountable agents. They can't be blamed in the human sense. Ah, okay. So there's a gap. Exactly. This creates what the guide calls an ethical breach or an accountability gap. The FAST-TRACK principles, which stand for fairness, accountability, sustainability, and transparency, are specifically designed to fill this gap. They aim to bridge the new smart agency of machines with their fundamental lack of\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  29%|██▉       | 43/149 [03:05<07:30,  4.25s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1260.00 --> 1290.00] moral responsibility. So they translate the need for moral accountability from humans into the design and deployment specs for these machines. That makes sense. And you mentioned earlier that these FAST principles aren't all quite the same type of principle. That's right. Accountability and transparency are seen as sort of end-to-end governing principles. They're about ensuring humans remain answerable and that the processes used are justifiable. Okay, the how. Right. Fairness and sustainability, on the other hand, are described more as qualities of algorithmic system.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  30%|██▉       | 44/149 [03:09<07:05,  4.05s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1290.00 --> 1320.00] Qualities for which the designers and implementers are held accountable through the mechanisms of transparency and accountability. They're all deeply interrelated, naturally, but their roles within the framework differ slightly. It's also important to note, as the guide does, that these principles often align closely with legal requirements, like the data protection principles in GDPR or the Data Protection Act 2018. That's a critical connection linking the ethical framework to existing legal obligations. Let's dive into the first of the fast.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  30%|███       | 45/149 [03:13<07:07,  4.11s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1320.00 --> 1350.00] principles. Fairness. This feels like the most immediate, maybe the most common concern when we think about AI and public services. Bias. Discrimination. It absolutely is. And the core challenge here, as the guide lays out, is that AI technologies are fundamentally human designed. So they're prone to human error, human prejudice, human bias at every single stage, from how data is extracted right through to how the system is implemented in the real world. And beyond that, the data itself, the stuff the AI learns from,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  31%|███       | 46/149 [03:17<07:05,  4.13s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1350.00 --> 1380.00] from often contains these culturally crystallized forms of bias and discrimination. History baked into the data. And the guide is very blunt. There's no silver bullet when it comes to fixing discrimination in AI. It's not just a simple technical problem you can patch. So what's the approach then? The guide proposes what it calls the principle of discriminatory non-harm as a minimum threshold. Basically, designers and users must prioritize bias mitigation. They have to actively work to ensure AI systems don't generate discriminatory\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  32%|███▏      | 47/149 [03:22<07:00,  4.12s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1380.00 --> 1410.00] or inequitable impacts. Okay, a minimum standard of do no discriminatory harm. And this principle implies focusing on specific areas, right? You mentioned data, design, outcomes, implementation. Yes, exactly. Fairness isn't just one thing. It has to be considered across the entire AI project pipeline. These four aspects are integrated. First, data fairness. This is absolutely foundational. Garbage in, garbage out, essentially. Pretty much. If the AI's results are generated by biased, compromised, or skewed data sets,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  32%|███▏      | 48/149 [03:26<07:12,  4.29s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1410.00 --> 1440.00] then the people affected by those results won't be protected from harm. So data fairness involves several things. Ensuring representativeness in the data doesn't reflect the population fairly. Making sure it's fit for purpose and sufficient. Is there enough relevant data? Checking its source integrity and measurement accuracy. Are we accidentally using biased historical decisions as ground truth? Ensuring it's timely and recent old data can introduce bias. And making sure it's relevant and appropriate, drawing on domain knowledge. That's a lot to check just for the data. Yeah.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  33%|███▎      | 49/149 [03:31<07:17,  4.38s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1440.00 --> 1470.00] It is. The guide suggests using something called a data set fact sheet. It's a best practice, basically. A log documenting the data's provenance, its quality checks, potential limitations. Okay, so documenting everything about the data. That makes sense for auditability later, too. What about design fairness? That's about the human choices made during development. Exactly. This focuses on the precautions needed to prevent bias creeping in to the human choices made across the workflow. This includes things like problem formulation. How do you translate a broad\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  34%|███▎      | 50/149 [03:35<07:02,  4.26s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1470.00 --> 1500.00] goal into specific measurable targets. You have to be careful that this translation doesn't inadvertently disadvantage certain groups. Right. How you define success matters. Hugely. It also involves data pre-processing. How do you label, annotate, or organize the training data? Human judgment is involved there, so it needs to be done with fairness in mind, using solid contextual info and metadata. And then there's feature determination and model building, selecting input variables, engineering new features, tuning the model's parameters. All these steps involve\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  34%|███▍      | 51/149 [03:39<06:49,  4.18s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1500.00 --> 1530.00] choices that could introduce bias. Awareness and often peer review are crucial here. The guide also warns about hidden proxies, things like redlining, where seemingly neutral data like zip codes actually correlate strongly with protected characteristics like race. So you have to scrutinize the correlations the model finds, not just assume they're okay, and demand interpretability, especially when dealing with social data. Precisely. If you can't explain how a correlation is morally justifiable and perceivable.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  35%|███▍      | 52/149 [03:43<06:39,  4.12s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1530.00 --> 1560.00] fair, you shouldn't be using it at high-stakes social systems. Which leads us to outcome fairness. Right. How do you define and measure if the results are fair? This is maybe the trickiest part, technically, because there are many different ways to define and measure fairness and outcomes. Some approaches focus on group fairness, like ensuring similar outcomes or error rates across different demographic groups, like demographic parity or equal opportunity. Others focus on individual\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  36%|███▌      | 53/149 [03:47<06:34,  4.11s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1560.00 --> 1590.00] fairness ensuring similar individuals are treated similarly. So different mathematical definitions of fairness. Exactly. And the guide is clear. There's no single best definition. They often involve trade-offs. Improving fairness by one metric might worsen it by another. And implementing them can be hard, sometimes requiring access to sensitive data. It requires a multidisciplinary effort to choose the right approach for the context. So you have to be upfront about it. Yes. The guide recommends creating a fairness position statement, or FPS. This would be a publicly available\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  36%|███▌      | 54/149 [03:51<06:24,  4.05s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1590.00 --> 1620.00] plain language explanation of which fairness criteria were chosen, why, and what the trade-offs are, transparency about the definition. Okay. Define your terms clearly. And finally, the fourth aspect, implementation fairness. This is about bias creeping in at the point of use. Right. Even if you designed a technically fair model, bias can still emerge when it's actually deployed and used by people in the real world. Automated decision support systems introduce some novel risks here. Like what? The guide highlights two main risks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  37%|███▋      | 55/149 [03:56<06:48,  4.35s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1620.00 --> 1650.00] main pitfalls. First, decision automation bias, which is sometimes called the technological halo effect. This is where human users tend to over-rely on or over-comply with the outputs of AI systems because they perceive them as objective or infallible. The computer said so, it must be right. Exactly that mentality. This can lead to what's called out-of-loop syndrome, where the human essentially stops critically engaging, their judgment degrades, and they might miss errors or perpetuate biases produced by the system. This can lead to safety hazards,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  38%|███▊      | 56/149 [04:00<06:40,  4.31s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1650.00 --> 1680.00] or discriminatory harm. So people just blindly trust the machine, potentially amplifying its mistakes? Precisely. The guide says the solution involves strong accountability regimes, proper training, and crucially framing the AI as assisting human judgment, not replacing it. Yeah. Humans need to stay in the loop, critically engaged. Okay. And the other pitfall? The other is kind of the opposite. Automation distressed bias. This is where users might disregard the AI's contributions, maybe due to general skepticism them or\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  38%|███▊      | 57/149 [04:04<06:15,  4.08s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1680.00 --> 1710.00] perhaps over-prioritizing their own expertise or intuition, even when the AI might be offering valuable insights. So either trusting it too much or not enough. Right. Both can lead to unfair or suboptimal outcomes. So safeguarding fair implementation means things like training the human implementers properly on the AI's limitations, designing the user interface to encourage active judgment, and proactively exploring potential cognitive biases in the users themselves. Fairness isn't a one-time check.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  39%|███▉      | 58/149 [04:09<06:39,  4.39s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1710.00 --> 1740.00] It's this continuous process integrated across the entire pipeline, enabling that end-to-end accountability. And that concept, end-to-end accountability, leads us perfectly into the next FAST principle, accountability. You mentioned that accountability gap earlier. What are the core challenges here, specifically for AI in the public sector context? Well, like we touched on, the first challenge is fundamental. AI systems themselves are not morally accountable agents. They can't answer for their actions. So, human answerability must be clearly attached to any answerable question.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  40%|███▉      | 59/149 [04:12<06:18,  4.20s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1740.00 --> 1770.00] decisions supported by AI. Someone has to be responsible. Okay, the buck has to stop with the human. All right. The second challenge is the sheer complexity of how AI gets produced and deployed. Think about all the different parties involved in a typical public sector AI project. Department leads making policy decisions, technical experts building the model, procurement specialists buying tools, policy advisors setting rules, frontline implementers actually using the system. Lots of cooks in the kitchen. Exactly. This multi-agent nature.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  40%|████      | 60/149 [04:16<06:03,  4.08s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1770.00 --> 1800.00] makes it incredibly difficult to assign responsibility if the system's use leads to negative consequences. Who exactly is accountable when so many hands touch the process? So it's not just that the AI itself isn't accountable, but also that figuring out which human should be accountable on such a complex web is really hard. How does the guide suggest breaking down accountability into more actionable concepts? It defines accountability mainly through two subcomponents, answerability and auditability. Answerability and auditability, okay. Answerability.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  41%|████      | 61/149 [04:21<06:09,  4.20s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1800.00 --> 1830.00] means establishing a continuous chain of human responsibility across that entire AI project workflow. No gaps where responsibility gets lost. It also demands that the relevant human authorities can offer understandable explanations and justifications for algorithmically supported decisions. And crucially, these explanations need to be in plain, coherent language based on sound and impartial reasons understandable to those affected. So someone has to be able to explain it clearly and take responsibility for it. What about auditability? Auditability and\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  42%|████▏     | 62/149 [04:24<05:44,  3.96s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1830.00 --> 1860.00] The answer is the how. How are designers and implementers actually held accountable? It means being able to demonstrate responsible practices and justifiable outcomes. This requires keeping detailed records, comprehensive logs. Logs of everything. Pretty much. Records for monitoring the soundness of the innovation processes, tracking data provenance from collection right through to deployment, and enabling critical review of how the system is actually operating in practice. The algorithmic models themselves must be built for audited\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  42%|████▏     | 63/149 [04:28<05:37,  3.92s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1860.00 --> 1890.00] accountability. They need to be reproducible and equipped for end-to-end recording and monitoring of their data processing. That dataset fact sheet we talked about earlier, that becomes a crucial piece of the audit trail. It sounds like the overall principle here is really accountability by design, building it in from the start, making it proactive rather than just reactive when something goes wrong. Yes, absolutely. The guide makes a distinction between anticipatory Accountability, or Ex-Ante, which prioritizes making good decisions before deployment.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  43%|████▎     | 64/149 [04:32<05:45,  4.07s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1890.00 --> 1920.00] to bolster the design's soundness, preempt potential harms, and inform stakeholders early. So getting it right beforehand. Right. And remedial accountability, or ex post, which focuses on corrective actions or providing justifications after a process is completed or a negative outcome occurs. The strong emphasis in the guide is on embedding accountability early, making it anticipatory to prevent problems in the first place. Though, of course, remedial accountability being able to justify things after the fact is still\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  44%|████▎     | 65/149 [04:36<05:43,  4.08s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1920.00 --> 1950.00] a necessary fallback. Okay, proactive accountability. From there, we move to the third FAST principle, sustainability. Now, this sounds like it's about the long-term view, the bigger picture impact. It is, exactly. Designers and users of AI systems need to remain constantly aware that these technologies can have truly transformative and long-term effects, both on individuals and on society as a whole. So to ensure sustainable deployment, project teams must be continuously sensitive to the real-world impacts their system will have or is having.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  44%|████▍     | 66/149 [04:41<05:40,  4.10s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1950.00 --> 1980.00] over time. So it's not just about whether it works now, but what its effects are down the line. How do they actually assess these long-term societal impacts? Is there a specific tool or process? Yes. The guide recommends using a stakeholder impact assessment, or SIA. SIA. This is basically a structured way to evaluate the potential social impact and the sustainability of an AI project. Yeah. It focuses on the effects on individuals, on different groups, on organizations. It applies whether the AI is for a public-facing or a public-facing.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  45%|████▍     | 67/149 [04:45<05:39,  4.14s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1980.00 --> 2010.00] service, or even just a back office function. And what's the purpose of doing an SIA? What does it achieve? The guide lists several purposes. Building public confidence and trust is a big one. Strengthening accountability, obviously. Helping to reveal unseen risks or unintended consequences before they happen. Underwriting more informed decision making by policymakers. And generally demonstrating forethought and due diligence, showing that the potential impacts have been seriously considered. So it's about anticipating and trying to address those broader...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  46%|████▌     | 68/149 [04:49<05:43,  4.24s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2010.00 --> 2040.00] ripple effects proactively, when exactly should these SIAs be done? Is it a one-off thing? No, definitely not a one-off. The guide recommends conducting SIAs or at least revisiting them at three critical points in the project lifecycle. Three points. Okay. First, early on in the alpha phase during the initial problem formulation stage. This is for an initial assessment of ethical permissibility, often using those SEM values we discussed. And if significant impacts are anticipated, this initial SIA should ideally be opened up to.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  46%|████▋     | 69/149 [04:54<05:41,  4.26s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2040.00 --> 2070.00] input from the public and internal stakeholders. Get feedback early. Makes sense. Second, from alpha to beta, which is basically pre-implementation. Once the model is trained and technically validated, the team revisits that initial SIA. They reconfirm its findings, update it based on the developed model, and crucially make it publicly available before the system is launched. They also need to set clear time frames for when it will be reassessed once the system is live. So transparency before launch and a plan for review. And the third point. The third point is the beta phase.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  47%|████▋     | 70/149 [04:58<05:35,  4.25s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2070.00 --> 2100.00] for reassessment. After the AI system is actually deployed and running in the real world, the team needs to intermittently revisit and re-evaluate the SIA. They compare the predicted impacts with the actual real-world impacts observed. This allows them to identify and mitigate any unintended negative consequences that have emerged, and it should incorporate further public consultation. It's designed to be a continuous feedback loop, constantly checking the system's real-world sustainability. A continuous loop of assessment and...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  48%|████▊     | 71/149 [05:02<05:23,  4.14s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2100.00 --> 2130.00] Adaptation. Okay. That covers societal sustainability. Now for the final aspect linked to sustainability in the FAST principles, which the guide sometimes discusses under safety or technical sustainability, ensuring the system itself is technically reliable and robust. This feels like it gets into the nuts and bolts of the AI system. It does. Beyond the broader social sustainability, this technical sustainability, let's call it safety for clarity here, as the guide often does, is absolutely paramount. A technically sustainable or safe AI system is one.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  48%|████▊     | 72/149 [05:05<05:02,  3.93s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2130.00 --> 2160.00] that is accurate, reliable, secure, and robust. Accurate, reliable, secure, robust. Right. Achieving these goals is challenging because AI often operates in a complex, uncertain world. But hitting these targets is crucial for mitigating risks and building that essential public trust, especially since potential failures in public sector AI could produce really harmful outcomes. So what are the core safety objectives teams should be aiming for when building these systems?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  49%|████▉     | 73/149 [05:10<05:12,  4.11s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2160.00 --> 2190.00] The guide breaks them down into those four operational objectives we just mentioned. First, accuracy and performance metrics. This is about the proportion of correct outputs versus errors, but it's also about refining those measures, maybe looking at the total cost of errors or using metrics like precision and recall. And critically, it's about coping with unavoidable noise in the data, architectural uncertainties in the model, and changes in the input data over time. So accuracy isn't static. No. Second, reliability. This simply means that the data is being sent to the server.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  50%|████▉     | 74/149 [05:14<05:07,  4.09s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2190.00 --> 2220.00] the ai system behaves exactly as its designers intended it to it consistently adheres to its program specifications does it do what it's supposed to do consistently okay third third security this covers a range of things protecting the ai system from adversarial attack someone trying to maliciously interfere with it maintaining the integrity of the information it processes ensuring its continuous functionality that it doesn't just crash and maintaining confidentiality where needed. Protecting it from bad actors and ensuring it stays.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  50%|█████     | 75/149 [05:17<04:51,  3.94s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2220.00 --> 2250.00] up. Exactly. And fourth, robustness. This is the goal that an AI system functions reliably and accurately, even under harsh or unexpected conditions. This could include deliberate adversarial intervention, errors made by the human implementer using the system, or even just the system encountering skewed or unusual data it wasn't specifically trained on. Can it handle surprises gracefully? Accurate, reliable, secure, robust. These objectives sound incredibly challenging to guarantee, especially with new\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  51%|█████     | 76/149 [05:21<04:47,  3.93s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2250.00 --> 2280.00] threats and complexities emerging all the time. What are some of the critical risks to AI safety that the guide specifically identifies? It flags several key risks. First, concept drift. We touched on this briefly. Most machine learning systems are static models trained on historical data. But the real world changes, right? The underlying data distributions can shift over time. So the past isn't always a good predictor of the future. Exactly. For example, a model trained on past disease patterns might start failing if a new variant emerged.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  52%|█████▏    | 77/149 [05:26<04:53,  4.08s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2280.00 --> 2310.00] emerges with different characteristics. Teams need vigilance and action plans to detect and address this drift, perhaps by retraining the model periodically. Ok, concept drift, what else? Second, brittleness. This refers to inherent limitations, particularly in very complex models like deep neural networks. They can often struggle when they encounter unfamiliar events or scenarios that fall outside their training data distribution. Because they lack genuine context awareness or common sense, they can make unexpected, sometimes bizarre, unexpected,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  52%|█████▏    | 78/149 [05:29<04:41,  3.97s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2310.00 --> 2340.00] unexplainable mistakes in these situations. Right. They might perform brilliantly on the test data, but fail spectacularly and unpredictably when faced with something genuinely new in the real world. Precisely. And this is especially critical in safety-critical applications. Think autonomous transportation, medical diagnosis. Unseen situations can lead to catastrophic failures if the model is too brittle. Okay. Brittleness. What's next? Third, adversarial attack. These are deliberate attempts to fool the AI.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  53%|█████▎    | 79/149 [05:33<04:38,  3.98s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2340.00 --> 2370.00] It involves maliciously modifying the input data, often in ways that are imperceptible to humans, specifically to induce the AI to make a mistake, a misclassification, or an incorrect prediction. Can you give an example? The classic example is slightly altering the pixels on an image of, say, a panda, so that a human still sees a panda. But a sophisticated image recognition AI confidently classifies it as a given. What? Or, more worryingly, changing a few pixels on a stop sign image so a self-driving car\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  54%|█████▎    | 80/149 [05:38<04:47,  4.17s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2370.00 --> 2400.00] as AI misidentifies it as a speed limit sign. Wow. That has huge safety implications for critical systems. Absolutely. It requires specific defense strategies, often called model hardening, like adversarial training or modifying the model architecture to be less susceptible, and runtime detection methods. Okay. Adversarial attacks. What else? Fourth, data poisoning. This is a related type of adversarial attack, but it happens earlier in the process. Here, an adversary compromises the data sources during collection or pre-processing.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  54%|█████▍    | 81/149 [05:42<04:39,  4.11s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2400.00 --> 2430.00] They deliberately manipulate the training data set itself to induce misclassification later on, or even systemic malfunction. Sometimes they create hidden back doors in the model. It really highlights the importance of responsible data management and securing your data pipeline. So sabotaging the training data itself, scary. Is there one more? Yes. Finally, the guide discusses misdirected reinforcement learning behavior, RL. Reinforcement learning is where systems learn by trial and error, trying to maximize a predefined reward function. Learning by doing. Right. However,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  55%|█████▌    | 82/149 [05:45<04:18,  3.86s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2430.00 --> 2460.00] If these systems lack broader context awareness or common sense, they might find clever but unintended and potentially harmful ways to optimize that reward function, ways that the designers didn't anticipate. For instance, an AI optimizing for traffic flow speed might learn to achieve that goal by, say, routing cars in a way that compromises pedestrian safety if pedestrian safety wasn't explicitly included and properly weighted in its reward function. So the RL system achieves its narrow goal.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  56%|█████▌    | 83/149 [05:50<04:24,  4.01s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2460.00 --> 2490.00] but causes unintended collateral damage along the way because it doesn't understand the bigger picture or human values. Precisely. It just optimizes the metric it was given. Mitigation strategies here include things like extensive simulations in safe environments before deployment, continuous monitoring in the real world, building in interpretability, and crucially, having human override or shutdown mechanisms. Human oversight is key. Always. The guide really emphasizes that end-to-end AI safety is paramount. These considerations, accuracy, reliability, security, robustness,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  56%|█████▋    | 84/149 [05:54<04:21,  4.03s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2490.00 --> 2520.00] they can't be an afterthought. They have to operate at every stage of the AI project life cycle, involving rigorous testing, validation, and ongoing self-assessments. Okay, we've covered a lot. The ethical groundwork with SUM values, the actionable fast principles, including fairness, accountability, sustainability, and safety. Now let's turn to the final fast principle, transparency. This feels absolutely vital for building public trust, especially when it's the government using AI. It is.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  57%|█████▋    | 85/149 [05:58<04:17,  4.02s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2520.00 --> 2550.00] Incredibly vital. And the guide does a good job of clarifying that transparency in the context of AI ethics actually has two distinct but closely related meanings. Two meanings. OK. On the one hand, transparency refers to interpretability. This is about knowing how and why a model performed the way it did. Understanding its internal logic, its rationale. It's often talked about as opening the black box of AI. It involves content clarification, making the system intelligible. So understanding the machine's inner workings, its logic, the why behind its output. Exactly.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  58%|█████▊    | 86/149 [06:02<04:12,  4.01s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2550.00 --> 2580.00] hand, transparency also refers to justifiability. This is broader. It means being able to demonstrate the ethical permissibility, the non-discrimination or fairness, and the trustworthiness of both the design and implementation processes that led to the AI and the actual outcomes produced by the AI system. Okay, so it's not just about understanding how the machine works, but also being able to understand and justify the human choices and processes behind it and whether the outcome itself is ethically sound. Precisely.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  58%|█████▊    | 87/149 [06:06<04:21,  4.22s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2580.00 --> 2610.00] It's about showing that the use of the AI is practically and morally sound and that the human decisions and governance surrounding it are also sound and defensible. So the guide breaks this down further using this process product distinction you mentioned. It outlines three critical tasks for ensuring transparent AI. Yes, that process product distinction is really helpful here. The three tasks are first process transparency. Task one, justify process. This involves demonstrating that those ethical principles, permissibility, non-discrimination, fairness, and justice are not just a matter of choice.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  59%|█████▉    | 88/149 [06:10<04:03,  3.99s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2610.00 --> 2640.00] safety were actively considered and applied throughout the design and implementation processes. You prove this by following best practices and having robust auditability measures in place. So, proving that the journey taken to arrive at the AI outcome was ethically sound and well-documented. Correct. Second, outcome transparency, task two, clarify content and explain outcome. This task requires showing, in plain, understandable language, the guide calls socially meaningful, how and why a specific model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  60%|█████▉    | 89/149 [06:13<03:50,  3.84s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2640.00 --> 2670.00] model performed as it did in a particular context or for a particular decision. It's about translating the complex technical rationale into everyday terms that non-specialists can actually grasp, making sense of it in terms of human practices and relevant societal factors. And the phrase socially meaningful is key, isn't it? The explanation has to resonate with human practices and societal context, not just be a list of mathematical functions. Absolute critical. And third, outcome transparency. Task three, justify outcome. This builds is\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  60%|██████    | 90/149 [06:18<04:04,  4.14s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2670.00 --> 2700.00] directly on the clarified content from task two. It demands demonstrating that a specific decision or behavior of the system is, in itself, ethically permissible, non-discriminatory or fair, and worthy of public trust. This justification relies heavily on having followed those sound processes established back in task one. That's a really clear three-part structure. Justify the process, explain the outcome, justify the outcome. So how does the guide suggest achieving that first part, process transparency? How do we make the governance itself transparent?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  61%|██████    | 91/149 [06:22<03:53,  4.03s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2700.00 --> 2730.00] It boils down to a few key elements. Maintaining strong regimes of professional and institutional transparency is the baseline. This means team members adhering to rigorous standards of integrity, honesty, impartiality, things you'd expect in public service anyway. And the design and implementation process itself should be as open to public scrutiny as reasonably possible, balancing transparency with necessary confidentiality limits. So openness and integrity are the foundation. And then you mentioned the PBG framework earlier. Yes, the process-based governance PBG framework is key here.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  62%|██████▏   | 92/149 [06:25<03:40,  3.87s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2730.00 --> 2760.00] Its purpose is to operationalize those values, SUM, and principles, FAST, across the entire AI design and development pipeline, whatever specific workflow model is being used, like CRISPM for data mining. Okay. It provides what the guide calls a landscape view of the governance procedures. It clearly shows who the relevant team members and roles are, what the necessary workflow stages are for specific ethical interventions or checks, sets explicit timeframes for follow-up actions, and defines clear\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  62%|██████▏   | 93/149 [06:30<03:48,  4.07s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2760.00 --> 2790.00] protocols for logging activity and ensuring that crucial end-to-end auditability. It's basically the detailed blueprint for how ethical considerations get woven into every step. And presumably all the information generated by following that PBG framework feeds into the auditability piece, enabling auditability with a process log. Exactly. The idea to centralize all this information digitally records from the PBG framework checkpoints, data about model development, testing results, SIA documents, fairness assessments. This creates a comprehensive process.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  63%|██████▎   | 94/149 [06:33<03:30,  3.83s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2790.00 --> 2820.00] And the benefit of that log? Two main benefits. First, it allows the team to demonstrate responsible practices and justifiable outcomes to any concerned parties, regulators, oversight bodies, the public. Second, and this is really important, it allows them to curate different kinds of explanations for different stakeholders based on their level of technical expertise. It helps ensure what the guide calls cognitive equity, making sure understanding is accessible, not just locked away in technical.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  64%|██████▍   | 95/149 [06:37<03:27,  3.85s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2820.00 --> 2850.00] jargon. Cognitive equity, that's a great term. Okay, that covers making the process transparent. Now let's tackle the other side, outcome transparency. Explaining and justifying the specific AI decisions. You said earlier there's no simple technological fix here, which sounds, well, challenging. It is challenging, yeah. Because it's a multifaceted undertaking. It's not just about technology. It requires sound human judgment, collaboration, and communication skills. The core challenge is to effectively clarify and convey the rationale behind a model's output.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  64%|██████▍   | 96/149 [06:41<03:21,  3.81s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2850.00 --> 2880.00] in a way that can properly inform human judgment downstream, and be genuinely accessible and meaningful to the stakeholders affected by the decision. Because without sufficient interpretability, without understanding why the model did what it did, it's impossible to tell how and why things might be going wrong, which is crucial for ensuring safety, checking for fairness, detecting hidden bias, and ultimately for building trust. So interpretable AI means making the logic understandable. And the guide connects this need for interpretability.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  65%|██████▌   | 97/149 [06:45<03:30,  4.05s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2880.00 --> 2910.00] back to four dimensions of human reasoning. Can you explain that? Yeah. This is a really interesting part of the guide. It argues that when we ask for an explanation of an algorithmic model's decision, we're essentially asking it to make explicit how the factors determining that outcome serve as evidence, almost as if a reasoning human had produced it. And human reasoning, the guide suggests, typically involves four interconnected dimensions. So what are they? First, aspects of logic, formal explanations, rules of inference. Second,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  66%|██████▌   | 98/149 [06:49<03:18,  3.89s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2910.00 --> 2940.00] Second, semantics understanding how and why things work, the technical rationale, the function of different parts. Third, social understanding grasping the context of human practices, beliefs, intentions. This is about clarifying the socially meaningful content. And fourth, moral justification, assessing rightness and wrongness, fairness, ethical permissibility. And these four dimensions map directly onto what we demand from AI interpretability. Exactly. The technical aspects of AI ensuring safety, robustness,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  66%|██████▋   | 99/149 [06:53<03:16,  3.93s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2940.00 --> 2970.00] fairness, require understanding the underlying logic and the semantic inner workings of the model. But because these AI systems are designed to fulfill human objectives and operate within our social world, we also need to understand their decisions in terms of their social context and meaning. And since their decisions often have direct moral consequences, especially in the public sector, their outcomes require moral justification. These dimensions are seen as interdependent, building on each other to form a complete explanation. That's a really holistic way to think about what constitutes a good explanation.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  67%|██████▋   | 100/149 [06:57<03:19,  4.06s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2970.00 --> 3000.00] Now, let's get into the more technical aspects of choosing, designing, and using an interpretable AI system. The guide makes an interesting point early on, saying that all AI models are, at a foundational level, mathematical glass boxes. What does that mean? Isn't the problem usually the black box? Right. What it means is that purely from a formal, logical perspective, any algorithmic model, even a massive deep neural network with millions of parameters, is technically\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  68%|██████▊   | 101/149 [07:01<03:11,  3.99s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3000.00 --> 3030.00] transparent. It's a closed system of computable operations. Rules and mathematical transformations are mechanically applied to inputs to determine outputs. In that very restricted mathematical sense, they are transparent. You can, in principle, trace every calculation. But crucially, that kind of mathematical transparency doesn't actually explain how or why a particular outcome occurred in a way that's meaningful or understandable to a human. It doesn't give you the intuitive grasp, the semantic understanding. And that's where the famous black box challenge really comes in.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  68%|██████▊   | 102/149 [07:05<03:03,  3.90s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3030.00 --> 3060.00] comes in. Yes, exactly. As machine learning systems handle more and more data and grow in complexity to capture intricate patterns, they use much larger feature spaces, meaning many more input variables and employ highly complex nonlinear mapping functions to connect inputs to outputs. This is characteristic of black box models like deep neural networks, support vector machines, or complex ensemble methods. So things get tangled up. Incredibly tangled. Yeah. The effects of changing one input variable become intertwined with many other variables.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  69%|██████▉   | 103/149 [07:09<03:07,  4.07s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3060.00 --> 3090.00] others in ways that are hard to isolate or understand intuitively. The internal logic becomes effectively opaque to human inspection. And this poses that key dilemma for designers and policymakers. How do you weigh the potential for superior performance or accuracy that these complex models might offer against the very real tangible risks of lacking interpretability, especially in critical public sector applications where fairness, safety, and accountability are paramount. It's a difficult trade-off and the guide offers specific guidelines to help.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  70%|██████▉   | 104/149 [07:13<03:02,  4.05s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3090.00 --> 3120.00] navigate this. What's the first guideline? Guideline one. Look first at context, potential impact, and domain-specific need. Before even thinking about specific models, you need to assess the situation. What's the type of application? Is it a low-stakes tool processing forms or a high-stakes safety critical system like a security checkpoint or medical diagnosis? The need for interpretability scales with the potential impact. Right. Context is king. Absolutely. And domain specificity is vital too. You need solid domain knowledge to\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  70%|███████   | 105/149 [07:17<02:56,  4.02s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3120.00 --> 3150.00] understand the sector-specific explanation standards, the expectations of stakeholders in that field, and maybe even how older existing technologies in that domain hand an explanation. Okay, so step one, understand the context, the risks, the domain requirements. What's guideline two? Guideline two, draw on standard interpretable techniques when possible. The message here is, don't jump straight to the complex black box if you don't need to. Teams should actively try to find the right fit between the risks, the data they have, and the available machine learning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  71%|███████   | 106/149 [07:22<02:56,  4.11s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3150.00 --> 3180.00] techniques. For applications with high impact, high safety criticality, or those processing sensitive personal data, simpler, inherently non-opaque techniques should be prioritized if they can achieve adequate performance. Like what kind of techniques? Things like decision trees, linear or logistic regression, if the number of features is manageable, rule lists models where the decision-making process is relatively straightforward to understand. The guide even suggests that reaching for a black box prematurely might sometimes be ineffective.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  72%|███████▏  | 107/149 [07:25<02:46,  3.96s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3180.00 --> 3210.00] efficient, adding unnecessary complexity and risk. So the mantra is use the simplest, most explainable model that can effectively do the job. But what if it can't? What if the problem genuinely requires a black box model to get the needed performance, maybe in areas like complex image recognition or natural language processing? That brings us logically to guideline three. When considering the use of black box AI systems, this guideline itself has three key steps. First, thoroughly weigh impacts and risks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  72%|███████▏  | 108/149 [07:29<02:36,  3.82s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3210.00 --> 3240.00] The decision to use a black box model should only be made if its potential impacts and risks have been carefully considered and deemed acceptable, and if supplemental interpretability tools can provide a sufficient level of semantic explainability enough to ensure the system operates safely, fairly, and ethically. So, use a black box only if you really have to, and only if you have robust methods to explain its behavior afterward. Okay, use with caution and only with explainability tools. What are these supplemental interventions?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  73%|███████▎  | 109/149 [07:33<02:42,  4.05s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3240.00 --> 3270.00] interpretability tools. That's the second step, right? Yes, second step. Consider options for supplemental interpretability tools. This involves assessing the range of technical methods available for providing post hoc explanations and choosing ones that fit the specific use case needs and provide clear explanatory resources. The guide explores several different strategies here. Can you give us a flavor of those strategies? Sure. There's internal explanation, which tries to make the internal components of the black box model itself more intelligible, perhaps for better engineering insight. There's external or post hoc explanation.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  74%|███████▍  | 110/149 [07:37<02:38,  4.06s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3270.00 --> 3300.00] This is probably the most common approach currently. It involves trying to understand the black box by observing its input-output behavior, essentially reverse engineering it. Techniques like perturbing the input slightly to see how the output changes or building simpler proxy models that mimic the black box's behavior locally. Tools like LIME and ESHAP fall into this category. They try to explain specific predictions by highlighting which input features were most important for that particular outcome. Lime and Esche. We hear those names.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  74%|███████▍  | 111/149 [07:41<02:24,  3.80s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3300.00 --> 3330.00] We do. They're prominent examples, though the guide notes they have limitations. The explanations are approximations and can sometimes be sensitive to how they're applied. Another strategy is supplemental explanatory infrastructure, building secondary explanatory facilities into the system itself. This might involve mechanisms that generate natural language explanations or justifications alongside the output, or visualization tools that help users understand the model's focus, like attention maps and image analysis.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  75%|███████▌  | 112/149 [07:45<02:24,  3.91s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3330.00 --> 3360.00] And finally, there's counterfactual explanation. Counterfactuals? What are those? This is a really interesting approach. Counterfactual explanations aim to provide actionable recourse to the person affected by the decision. They answer the question, what factors would need to change and by how much for the outcome to be different and presumably more favorable, for example. Your loan application would have been approved if your income was X amount higher. Wow, that sounds incredibly useful and empowering for the individual, giving them\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  76%|███████▌  | 113/149 [07:50<02:29,  4.16s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3360.00 --> 3390.00] concrete steps they could potentially take. It can be, yes. It's powerful because it's contrastive and agency-promoting. Though again, the guide notes limitations generating them can be complex. They might not capture all interactions, and they work best when only a few factors need changing. And these different strategies also relate to the scope of the explanation, right? The guide talks about local versus global interpretability. Correct. That's an important distinction. Local interpretability focuses on explaining individual cases or Thank you.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  77%|███████▋  | 114/149 [07:53<02:23,  4.10s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3390.00 --> 3420.00] predictions. LIME and S-SHOP are primarily local methods. They tell you why the specific prediction was made. Global interpretability, on the other hand, aims to explain the behavior of the model as a whole. How does it generally make decisions? What are the overall patterns it has learned? Understanding the big picture of the model. Right. Achieving true global understanding of a complex black box is often very difficult. But the guide suggests constructive views. Even approximating the opaque model with a simpler interpretable one can provide value.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  77%|███████▋  | 115/149 [07:57<02:16,  4.02s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3420.00 --> 3450.00] valuable insights into its general behavior, support knowledge discovery, and help advance data science by revealing patterns. Okay. So, weigh risks, consider explanation tools, local, global, counterfactual. What's the third step under Guideline 3? The third step is to formulate an interpretability action plan. Once you've decided on explanatory strategies, you need a concrete plan. Articulate which tools and techniques will be used, specify the delivery strategy, how explanations will be provided to different users,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  78%|███████▊  | 116/149 [08:02<02:23,  4.36s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3450.00 --> 3480.00] set time frames for evaluating the effectiveness of the explanations, and clearly map out who is responsible for what in delivering interpretability. A concrete plan. That makes sense. This all sounds like a continuous, thoughtful effort. And the guide's final guideline, guideline four, think about interpretability in terms of the capacities of human understanding, seems like a really crucial overarching principle tying this together. It absolutely is. It reminds us that interpretability isn't some binary switch into either interpretable or not. It's a continuum of comprehensibility.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  79%|███████▊  | 117/149 [08:06<02:08,  4.02s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3480.00 --> 3510.00] And where something falls on that continuum depends fundamentally on human cognitive limits, meaning even a mathematically simple model, like a linear regression, can become effectively uninterpretable if it involves, say, thousands of features. No human can hold all those weighted factors in their head and intuitively grasp the overall logic. The sheer volume of information exceeds our cognitive capacity. So complexity kills interpretability, even for simple models sometimes. Yes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  79%|███████▉  | 118/149 [08:10<02:03,  3.97s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3510.00 --> 3540.00] And the key takeaway from this guideline is the need to tailor explanations to the cognitive capacities and expertise levels of different audiences. Technical operators running the system need a different kind of explanation than frontline implementers using its outputs, who in turn need something different from the non-specialist members of the public affected by its decisions. Ensuring this cognitive equity, making sure understanding is genuinely accessible to all relevant parties, is vital for responsible AI. Cognitive equity. I really like that framing. It's not just about technology.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  80%|███████▉  | 119/149 [08:14<02:03,  4.13s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3540.00 --> 3570.00] technical transparency, but about making that transparency meaningful to everyone involved. It sounds like all these guidelines, all this thinking about interpretability, ultimately culminates in how you actually implement these systems in a human-centered way. Precisely. The final major section of the guide, securing responsible delivery through human-centered implementation protocols and practices, is entirely dedicated to this crucial last mile. The core principle here is straightforward, but powerful. You must devise delivery and implementation process.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  81%|████████  | 120/149 [08:18<01:57,  4.07s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3570.00 --> 3600.00] that are deeply sensitive to human factors, to people's needs, their competences, their capacities, their contexts. To provide clear and effective explanations that actually work in practice, you have to build from the human ground up, understanding the specific use case and domain context inside out. Building from the human ground up, okay. And the guide outlines three steps for achieving this human-centered implementation. What's step one? Step one. Consider aspects of system type and domain context to define roles and determine user needs.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  81%|████████  | 121/149 [08:21<01:48,  3.89s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3600.00 --> 3630.00] needs. This involves really thinking about who will be affected by the system. Which communities? Are there vulnerable groups involved? How might their background or experiences influence their ability to understand or challenge an AI-driven decision? The explanatory strategy needs to be fine-tuned to accommodate these varying needs, providing clear, non-technical details accessible to everyone. So, map out the users and their needs? Yes, and define their roles clearly. The guide gives a hypothetical example.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  82%|████████▏ | 122/149 [08:26<01:53,  4.19s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3630.00 --> 3660.00] of a predictive risk assessment system in, say, social care. It defines roles like the decision subject, the person the assessment is about, their advocate, maybe a lawyer or support worker, the implementer, the frontline social worker using the tool, the system operator, technical staff managing the system, and the delivery manager overseeing the service. Each role has different goals, objectives, and critically different levels of technical and domain knowledge. Their needs for explanation vary significantly. That's a really practical way to break down the complex user landscape. Define the role of the user landscape.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  83%|████████▎ | 123/149 [08:30<01:45,  4.06s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3660.00 --> 3690.00] understand their specific needs, what's step two? Step two, define delivery relations and map delivery processes. Once you have the roles, you need to think about the relationships between them. How does the decision subject or their advocate interact with the implementer? How does the implementer get technical support or clarification from the system operator? Mapping these relationships and the communication flows is crucial. Okay. And the principal objective of the delivery process itself, the guide argues, is twofold.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  83%|████████▎ | 124/149 [08:34<01:38,  3.94s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3690.00 --> 3720.00] First, to translate the statistically expressed results from the AI into humanly significant reasons. Second, to translate the algorithmic outputs into socially meaningful outcomes within the context of the person's life and the service being provided. So it's not enough to just hand over the raw data or probability score. You have to translate it into human terms, into social context. Exactly. It's a translation task. And this twofold objective reasons and meaningful outcomes helps organize the duties of responsible implementation into a\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  84%|████████▍ | 125/149 [08:37<01:31,  3.81s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3720.00 --> 3750.00] technical component and a social component. Technical and social components. Tell us about the technical component first. The technical component is about conveying the statistical results appropriately as empirically derived evidence that can support but not replace sound human judgment. AI results, the guide says, serve as stand-ins for acts of speech and representation. And like any representation making a claim, they require rational justification. So treat the AI output like evidence.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  85%|████████▍ | 126/149 [08:42<01:32,  4.01s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3750.00 --> 3780.00] that needs to be weighed. Right. Delivering this technical content responsibly means prioritizing the perspectives of the user and the decision subject, getting input from domain experts. It often requires a multi-tiered approach based on technical literacy. Implementers need to understand the model's assumptions and limitations. System operators need deeper technical proficiency. Decision subjects need clarity above all. And what kind of technical information should be delivered? The guide makes several recommendations. Use plain, non-technical...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  85%|████████▌ | 127/149 [08:45<01:24,  3.84s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3780.00 --> 3810.00] language for implementers and decision subjects. Present results as facts or evidence supporting a conclusion with clear premises and rationale. Make process logs and monitoring results available. And crucially, include that implementation disclaimer we discussed earlier. Yes, that disclaimer felt really important, worth reiterating its core message. Absolutely. The essence is, this AI output is intended to assist your judgment, not replace it. It's not the sole basis for your decision. It's based on\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  86%|████████▌ | 128/149 [08:49<01:17,  3.69s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3810.00 --> 3840.00] statistical analysis of population data, meaning there's unavoidable uncertainty, and it doesn't capture the specific circumstances of the individual. Your inference must bridge that gap from statistical generalization to the individual's concrete situation. That's such a vital qualification, statistical generalization versus individual reality. It is. Other technical recommendations include providing explicit performance metrics, clear representations of confidence intervals or error bars, raw numerical probabilities behind any simplified score.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  87%|████████▋ | 129/149 [08:53<01:18,  3.95s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3840.00 --> 3870.00] like wristbands 110, and including counterfactual explanatory tools where feasible to offer that actionable recourse. Okay, that's the technical side. What about the social component of responsible implementation? You said it's about translating into socially meaningful terms. Yes, this is about clarifying the socially meaningful content embedded within and produced by the AI's output. It means explicitly translating the model's technical machinery, its variables, parameters, functions, back into the everyday language of humanly relevant.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  87%|████████▋ | 130/149 [08:57<01:13,  3.86s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3870.00 --> 3900.00] meanings, categories, and relationships that actually shaped its original design in the first place. So connecting the math back to the human world it came from. Precisely. It's this reconstruction of the underlying societal purposes, norms, and practices that allows implementers to apply the AI's insights appropriately to the specific social and individual contexts of the people affected. It requires training implementers to carry out this translation with, as the guide puts it. Interpretive charity, reasonableness, empathy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  88%|████████▊ | 131/149 [09:01<01:10,  3.90s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3900.00 --> 3930.00] and context specificity. That sounds less like technical training and more like training and interpretation and communication. Understanding the human story behind the data points. It really is. The guide visualizes this as the content life cycle, which has three phases. Phase one, translate human values in during the design process. This is where human choices about goals, data sampling, feature engineering happen. Right. And where historical biases might get embedded in the data. Okay. Values and biases go in. Phase two, data processing. This is the.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  89%|████████▊ | 132/149 [09:05<01:07,  3.97s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3930.00 --> 3960.00] computation stage where the trained model processes the quantified proxies of those human purposes and values. The machine does its thing. Right. Phase three. Translate human values out during the implementation process. This is where the implementer reconstructs the societal purposes and norms to clarify the socially meaningful content of the results for this specific situation. And the guide argues it's only through this crucial retranslation out that the model becomes usefully interpretable and enables genuine end-to-end accountability. Translate it.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  89%|████████▉ | 133/149 [09:09<01:03,  3.94s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3960.00 --> 3990.00] process translate out and this leads to something called the translation rule yes the translation rule it basically states what is translated in to an algorithmic system is directly proportional to what in terms of the explanatory needs must be translated out so the more complex the human values and social context you try to capture in the model the more explanation and interpretation is required out at the implementation stage exactly the rule highlights two key distinctions first between clarifying socially meaningful you\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  90%|████████▉ | 134/149 [09:13<00:58,  3.90s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3990.00 --> 4020.00] content, which makes AI interpretable, and establishing normative rightness, which makes AI justifiable. Second, between intention and design, the human purpose is baked into the model's construction, and intention and application, how the human implementer uses the output and the consequences for the subject. And the guide uses two contrasting examples to really drive this home, doesn't it? It does, very effectively. It compares a hypothetical radiomics cancer detection system with a predictive risk assessment system in child social care. Okay.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  91%|█████████ | 135/149 [09:16<00:54,  3.88s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4020.00 --> 4050.00] How do they differ in terms of this translation rule? Well, in the radiomic system, the social content translated in is relatively minimal. It's primarily about clinical goals, image features related to pathology. But the normative aspect, patient well-being, safety, avoiding misdiagnosis, is highly significant. So the main explanatory burden translation out for the physician and patient is less about unpacking complex social meanings and more about justifying the system's safety, reliability, accuracy, and ensuring equitable access to this health benefit.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  91%|█████████▏| 136/149 [09:20<00:51,  3.93s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4050.00 --> 4080.00] Makes sense. Simpler social input, high safety justification needed. But the child social care example must be completely different. Entirely different. The social content translated in is incredibly intricate and extensive. The target variable itself, child safety, is complex. The features might include things like age, public health records, family history, parental mental health, neighborhood crime rates, all deeply socially embedded concepts. Loaded with context. Exactly.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  92%|█████████▏| 137/149 [09:24<00:46,  3.86s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4080.00 --> 4110.00] The affirmative aspect is also extremely complex, balancing the paramount need for child welfare against principles of parental fairness, system efficiency, the sanctity of familial relations. So the translation burden on the frontline social worker using this tool is immense. What do they need to do? They have to scrutinize the particular decision subject situation intensely. They need to clarify the socially meaningful content of all those input features, like demographic info or past welfare interactions, within the living context of that specific family. And\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  93%|█████████▎| 138/149 [09:28<00:41,  3.75s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4110.00 --> 4140.00] And they must critically assess the potential legacies of historical discrimination or structural bias that might be embedded in that data or the model's logic. The guide calls this process of weighing the algorithmic output against the specific human context and broader ethical principles normative triangulation. It's crucial for achieving justifiable AI in such sensitive domains. Normative triangulation. Wow. So the social worker becomes this essential bridge, this interpreter between the algorithm's statistical pattern matching and the incredibly nuanced.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  93%|█████████▎| 139/149 [09:33<00:41,  4.12s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4140.00 --> 4170.00] high-stakes human reality. It really underscores how the AI is just a tool, and the ultimate responsibility, the deep interpretation, rests squarely with trained, ethically aware people. Absolutely. It cannot replace that situated human judgment, which leads us to the final step in human-centered implementation. Step three, build an ethical implementation platform. This sounds like the capstone, bringing it all together. Yes, that's exactly what it is. It has several components. First, train ethical implementation. Implementers need to be prepared\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  94%|█████████▍| 140/149 [09:36<00:35,  3.96s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4170.00 --> 4200.00] not just technically, but ethically. They need to be stewards of interpretable and justifiable AI. What does that training involve? It involves equipping them to rationally evaluate the AI outputs, communicate algorithmically-assisted decisions effectively in plain language, appropriately apply the system's conclusions to specific social circumstances, weigh the AI insights against broader purposes and values, and ultimately justify the ethical permissibility and trustworthiness of both the age and the time.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  95%|█████████▍| 141/149 [09:40<00:32,  4.02s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4200.00 --> 4230.00] AI systems outcomes, and the processes behind them. So it's a demanding role requiring ethical and communicative skills, not just technical know-how. So training for judgment and communication, what else is part of the platform? Second, make your implementation platform a relevant part and capstone of the sustainability track of your project. This means the platform shouldn't just be about pushing information out. It needs to be a two-way street. It should serve as a sounding board for feedback from implementers and affected communities about the system's real-life effects, its usability,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  95%|█████████▌| 142/149 [09:44<00:28,  4.02s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4230.00 --> 4260.00] its unforeseen consequences. It needs to be dialogically and collaboratively connected to the communities it serves to support that ongoing SIA process we talked about. A feedback loop for continuous improvement. Exactly. And third, provide a model sheet to implementers and establish protocols for implementation reporting. This means giving implementers concise summary sheets, model sheets containing key information, technical specs, performance metrics, the fairness criteria used, that crucial implementation disclaimer,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  96%|█████████▌| 143/149 [09:49<00:24,  4.08s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4260.00 --> 4290.00] and easy links to the full process logs and SIA information. You also need clear protocols for reporting issues, successes, or concerns, with the level of reporting proportional to the risks involved. Giving them the key info at their fingertips and a way to report back makes sense. And finally... Finally, and perhaps most importantly, foster outcome understanding through dialogue. This really ties everything together. The guide stresses that achieving interpretable and justifiable AI is inherently a dialogical and collaborative effort. Understanding is not just about understanding.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  97%|█████████▋| 144/149 [09:52<00:19,  3.95s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4290.00 --> 4320.00] isn't just passively received, it's actively reached through communication. A conversation. Yes. Implementers and decision subjects, or their advocates, are participants in an ongoing explanatory dialogue. Understanding is achieved through open, respectful, sincere, and well-informed communication where all voices are considered. This dialogue not only advances responsible implementation in specific cases, but also encourages reflection and feedback that can lead to improvements in the AI's design, delivery, and overall performance over time.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  97%|█████████▋| 145/149 [09:56<00:15,  4.00s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4320.00 --> 4350.00] It brings the whole process full circle. Wow. What an incredible, detailed, deep dive. We've really journeyed through the complexities of AI ethics and safety, specifically from that government perspective, trying to shape these AI-powered economies responsibly, from foundational SDRM values right down to practical implementation protocols and the importance of dialogue. It is a complex landscape, definitely. But what's clear from the Alan Turing Institute guide is that there is a path forward. It provides a comprehensive and I think quite actionable roadmap.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  98%|█████████▊| 146/149 [10:01<00:12,  4.25s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4350.00 --> 4380.00] roadmap for navigating these challenges responsibly, always keeping that focus on public benefit. It really makes you think, doesn't it? Going right back to the origins of this technology. You know, back in 1936, Alan Turing himself, with just pencil and paper, sketched out the abstract idea of the Turing machine, fundamentally defining what it means to compute. Yeah, an incredibly humble beginning for something that ushered in the entire digital age. And led eventually to these omnipresent algorithmic systems we've been discussing today. And as the guide points out quite powerfully,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  99%|█████████▊| 147/149 [10:05<00:08,  4.03s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4380.00 --> 4410.00] near the end, AI isn't just another general purpose technology like, say, electricity. It's evolving into something more. A gatekeeper technology. Gatekeeper. There's that phrase again. Yeah. Meaning it uniquely holds the key, potentially, both to exponential advancements in human well-being and simultaneously to significant, maybe even existential, risks for society's future. It controls access to so many potential futures. So it really leaves us as a species with.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks:  99%|█████████▉| 148/149 [10:09<00:04,  4.00s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4410.00 --> 4440.00] a fundamental choice. The guide, as you said, has offered one way to move forward, one framework. It suggests that responsible innovation comes from deliberately prioritizing ethical purposes and values. It's about consciously trying to steer the course of our algorithmic creations in accordance with a shared vision of what a better human future should look like, not just letting the tech lead us wherever it may go. Ultimately, the guide reminds us, and this is a powerful closing thought, that it is humankind\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Chunks: 100%|██████████| 149/149 [10:12<00:00,  3.68s/it]\u001b[A\n",
            "Transcribing audio files: 100%|██████████| 1/1 [10:31<00:00, 631.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4440.00 --> 4455.84] that must ultimately choose which direction the key will turn. The choice is ours. The technology doesn't choose for us. That really is a provocative thought to leave you, our listeners, with. How will you, in your own way, contribute to shaping this future? Which direction will you help turn that key?\n",
            "\n",
            "✅ Saved transcript to: /content/drive/MyDrive/AI Governance.wav.txt\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Imports\n",
        "import os\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n",
        "from tqdm import tqdm\n",
        "\n",
        "# STEP 2: Set device\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "print(f\"\\nDevice set to use {device}\")\n",
        "\n",
        "# STEP 3: Load model & processor\n",
        "model_id = \"openai/whisper-large-v3\"\n",
        "\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# STEP 4: Define parameters\n",
        "chunk_duration = 30.0  # seconds\n",
        "generate_kwargs = {\n",
        "    \"max_length\": 448,\n",
        "    \"return_timestamps\": \"word\",  # optional: \"word\" or \"segment\"\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "\n",
        "# STEP 5: Define input directory\n",
        "input_dir = '/content/drive/MyDrive/'  #@param {type: \"string\"}\n",
        "\n",
        "# STEP 6: Find audio files\n",
        "audio_files = [\n",
        "    os.path.join(input_dir, f)\n",
        "    for f in os.listdir(input_dir)\n",
        "    if f.endswith('.mp3') or f.endswith('.wav')\n",
        "]\n",
        "\n",
        "# STEP 7: Transcribe files\n",
        "for audio_path in tqdm(audio_files, desc=\"Transcribing audio files\"):\n",
        "    print(f\"\\n🗂️ Transcribing: {audio_path}\")\n",
        "\n",
        "    # Load audio\n",
        "    audio_array, sr = librosa.load(audio_path, sr=16000)\n",
        "    total_duration = librosa.get_duration(y=audio_array, sr=sr)\n",
        "    num_chunks = int(np.ceil(total_duration / chunk_duration))\n",
        "\n",
        "    print(f\"📏 Total duration: {total_duration:.2f} sec → {num_chunks} chunks of {chunk_duration:.0f} sec\")\n",
        "\n",
        "    full_transcript = \"\"\n",
        "\n",
        "    for i in tqdm(range(num_chunks), desc=\"Chunks\", leave=False):\n",
        "        start_sample = int(i * chunk_duration * sr)\n",
        "        end_sample = int(min((i + 1) * chunk_duration * sr, len(audio_array)))\n",
        "        chunk = audio_array[start_sample:end_sample]\n",
        "\n",
        "        # Process this chunk\n",
        "        inputs = processor(chunk, sampling_rate=16000, return_tensors=\"pt\")\n",
        "        input_features = inputs.input_features.to(device, dtype=torch_dtype)\n",
        "\n",
        "        # Generate\n",
        "        generated_tokens = model.generate(input_features=input_features, **generate_kwargs)\n",
        "        decoded = processor.batch_decode(generated_tokens, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "        # Print live chunk result\n",
        "        start_time = i * chunk_duration\n",
        "        end_time = min((i + 1) * chunk_duration, total_duration)\n",
        "        print(f\"[{start_time:06.2f} --> {end_time:06.2f}] {decoded}\")\n",
        "\n",
        "        # Append to full transcript\n",
        "        full_transcript += f\"[{start_time:.2f} --> {end_time:.2f}] {decoded}\\n\"\n",
        "\n",
        "    # Save transcript to .txt\n",
        "    transcript_path = audio_path + \".txt\"\n",
        "    with open(transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(full_transcript)\n",
        "    print(f\"\\n✅ Saved transcript to: {transcript_path}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}