{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFtEeY13V5le"
      },
      "source": [
        "# Setup\n",
        "- Upload audio mp3 file/s to Google Drive\n",
        "- Change runtime type Hardware accelerator to T4 GPU\n",
        "- Install Whisper and mount Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNmYH7tsGWkU"
      },
      "source": [
        "# whisper-large-v2\n",
        "- [https://github.com/openai/whisper](https://github.com/openai/whisper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-oYXJ64VDD0"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d6CY0USYMq8"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpoREdPV2bkD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the input directory containing audio files\n",
        "input_dir = '/content/drive/MyDrive/'  #@param {type: \"string\"}\n",
        "\n",
        "# Iterate over audio files in the input directory\n",
        "for audio_file in os.listdir(input_dir):\n",
        "    if audio_file.endswith('.mp3'):  # Adjust the file extension as needed\n",
        "        audio_path = os.path.join(input_dir, audio_file)\n",
        "\n",
        "        # Transcribe the audio file\n",
        "        !whisper \"{audio_path}\" --model large-v3 --output_dir \"{input_dir}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTKIj0FQGmBV"
      },
      "source": [
        "# whisper-large-v3\n",
        "- https://huggingface.co/openai/whisper-large-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqKDaX6iGxdK"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install --upgrade pip\n",
        "!pip install torch==2.6.0+cu118 torchaudio==2.6.0+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -U transformers librosa soundfile tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZJfWqMjG93z"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xs6xd29mcI1"
      },
      "outputs": [],
      "source": [
        "# STEP 0: (Before importing transformers) disable torchvision usage inside transformers\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TORCHVISION\"] = \"1\"\n",
        "\n",
        "# STEP 1: Imports\n",
        "import torch, librosa, numpy as np\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# STEP 2: Set device\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "print(f\"\\nDevice set to use {device}\")\n",
        "\n",
        "# STEP 3: Load model & processor\n",
        "model_id = \"openai/whisper-large-v3\"\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ").to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# STEP 4: Parameters\n",
        "chunk_duration = 30.0  # seconds\n",
        "generate_kwargs = {\n",
        "    \"max_length\": 448,\n",
        "    \"return_timestamps\": \"word\",\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "\n",
        "# STEP 5: Input directory\n",
        "input_dir = '/content/drive/MyDrive/'  # @param {type: \"string\"}\n",
        "\n",
        "# STEP 6: Find audio files\n",
        "audio_files = [\n",
        "    os.path.join(input_dir, f)\n",
        "    for f in os.listdir(input_dir)\n",
        "    if f.lower().endswith(('.mp3', '.wav', '.flac', '.m4a'))\n",
        "]\n",
        "\n",
        "# STEP 7: Transcribe\n",
        "for audio_path in tqdm(audio_files, desc=\"Transcribing audio files\"):\n",
        "    print(f\"\\n🗂️ Transcribing: {audio_path}\")\n",
        "\n",
        "    audio_array, sr = librosa.load(audio_path, sr=16000)\n",
        "    total_duration = librosa.get_duration(y=audio_array, sr=sr)\n",
        "    num_chunks = int(np.ceil(total_duration / chunk_duration))\n",
        "\n",
        "    print(f\"📏 Total duration: {total_duration:.2f} sec → {num_chunks} chunks of {chunk_duration:.0f} sec\")\n",
        "\n",
        "    full_transcript = []\n",
        "\n",
        "    for i in tqdm(range(num_chunks), desc=\"Chunks\", leave=False):\n",
        "        start_sample = int(i * chunk_duration * sr)\n",
        "        end_sample = int(min((i + 1) * chunk_duration * sr, len(audio_array)))\n",
        "        chunk = audio_array[start_sample:end_sample]\n",
        "\n",
        "        inputs = processor(chunk, sampling_rate=16000, return_tensors=\"pt\")\n",
        "        input_features = inputs.input_features.to(device, dtype=torch_dtype)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            generated_tokens = model.generate(input_features=input_features, **generate_kwargs)\n",
        "\n",
        "        decoded = processor.batch_decode(generated_tokens, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "        start_time = i * chunk_duration\n",
        "        end_time = min((i + 1) * chunk_duration, total_duration)\n",
        "        line = f\"[{start_time:06.2f} --> {end_time:06.2f}] {decoded}\"\n",
        "        print(line)\n",
        "        full_transcript.append(line)\n",
        "\n",
        "    transcript_path = audio_path + \".txt\"\n",
        "    with open(transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(full_transcript))\n",
        "    print(f\"\\n✅ Saved transcript to: {transcript_path}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}