{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Qwen3-TTS Demo\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kurtvalcorza/notebooks/blob/main/Qwen3_TTS.ipynb)\n",
                "\n",
                "This notebook runs the [Qwen3-TTS](https://huggingface.co/spaces/Qwen/Qwen3-TTS) demo.\n",
                "\n",
                "### Notes\n",
                "- **Runtime**: GPU is required (T4 is sufficient).\n",
                "- **Dependencies**: Installs `flash-attn` if possible, but falls back to standard attention if needed.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 1. Setup and Installation\n",
                "#@markdown This step clones the repository and installs necessary dependencies.\n",
                "\n",
                "import os\n",
                "import subprocess\n",
                "import sys\n",
                "\n",
                "# Clone the repository\n",
                "if not os.path.exists(\"Qwen3-TTS\"):\n",
                "    !git clone https://huggingface.co/spaces/Qwen/Qwen3-TTS\n",
                "\n",
                "%cd Qwen3-TTS\n",
                "\n",
                "# Install dependencies\n",
                "# Removing 'spaces' as it is HF specific and we will mock it\n",
                "%pip install -r requirements.txt\n",
                "%pip uninstall -y spaces\n",
                "\n",
                "# Install ffmpeg for audio processing\n",
                "!apt-get install -y ffmpeg\n",
                "\n",
                "print(\"Setup complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 2. Mocking HF Spaces\n",
                "#@markdown We replace the `spaces` library with a dummy implementation since we are running on a dedicated Colab GPU.\n",
                "\n",
                "import sys\n",
                "from functools import wraps\n",
                "\n",
                "# Mock the spaces module\n",
                "class MockSpaces:\n",
                "    def GPU(self, duration=60):\n",
                "        def decorator(func):\n",
                "            @wraps(func)\n            def wrapper(*args, **kwargs):\n",
                "                return func(*args, **kwargs)\n",
                "            return wrapper\n",
                "        return decorator\n",
                "\n",
                "sys.modules[\"spaces\"] = MockSpaces()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 3. Load Models and Define Functions\n",
                "#@markdown This cell loads the model logic adapted from `app.py`.\n",
                "\n",
                "import os\n",
                "import gradio as gr\n",
                "import numpy as np\n",
                "import torch\n",
                "from huggingface_hub import snapshot_download, login\n",
                "\n",
                "# Login is optional for public models, but good practice if you have a token\n",
                "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
                "if HF_TOKEN:\n",
                "    login(token=HF_TOKEN)\n",
                "\n",
                "# Global model holders\n",
                "loaded_models = {}\n",
                "MODEL_SIZES = [\"0.6B\", \"1.7B\"]\n",
                "\n",
                "def get_model_path(model_type: str, model_size: str) -> str:\n",
                "    return snapshot_download(f\"Qwen/Qwen3-TTS-12Hz-{model_size}-{model_type}\")\n",
                "\n",
                "def get_model(model_type: str, model_size: str):\n",
                "    global loaded_models\n",
                "    key = (model_type, model_size)\n",
                "    if key not in loaded_models:\n",
                "        # The repo expects 'qwen_tts' to be importable. Since we are in the root of the cloned repo,\n",
                "        # it should work. if not, we might need to append sys.path\n",
                "        try:\n",
                "            from qwen_tts import Qwen3TTSModel\n",
                "        except ImportError:\n",
                "            sys.path.append(os.getcwd())\n",
                "            from qwen_tts import Qwen3TTSModel\n",
                "\n",
                "        model_path = get_model_path(model_type, model_size)\n",
                "        \n",
                "        # Check for flash attention\n",
                "        attn_implementation = \"eager\"\n",
                "        try:\n",
                "            import flash_attn\n",
                "            attn_implementation = \"flash_attention_2\"\n",
                "        except ImportError:\n",
                "            pass\n",
                "            \n",
                "        print(f\"Loading {model_type} {model_size} with {attn_implementation}...\")\n",
                "        loaded_models[key] = Qwen3TTSModel.from_pretrained(\n",
                "            model_path,\n",
                "            device_map=\"cuda\",\n",
                "            dtype=torch.bfloat16,\n            token=HF_TOKEN,\n",
                "            attn_implementation=attn_implementation\n",
                "        )\n",
                "    return loaded_models[key]\n",
                "\n",
                "def _normalize_audio(wav, eps=1e-12, clip=True):\n",
                "    x = np.asarray(wav)\n",
                "    if np.issubdtype(x.dtype, np.integer):\n",
                "        info = np.iinfo(x.dtype)\n",
                "        if info.min < 0:\n",
                "            y = x.astype(np.float32) / max(abs(info.min), info.max)\n        else:\n            mid = (info.max + 1) / 2.0\n            y = (x.astype(np.float32) - mid) / mid\n    elif np.issubdtype(x.dtype, np.floating):\n        y = x.astype(np.float32)\n        m = np.max(np.abs(y)) if y.size else 0.0\n        if m > 1.0 + 1e-6:\n            y = y / (m + eps)\n    else:\n        raise TypeError(f\"Unsupported dtype: {x.dtype}\")\n    if clip:\n        y = np.clip(y, -1.0, 1.0)\n    if y.ndim > 1:\n        y = np.mean(y, axis=-1).astype(np.float32)\n    return y\n",
                "\n",
                "def _audio_to_tuple(audio):\n",
                "    if audio is None:\n",
                "        return None\n",
                "    if isinstance(audio, tuple) and len(audio) == 2 and isinstance(audio[0], int):\n",
                "        sr, wav = audio\n",
                "        wav = _normalize_audio(wav)\n",
                "        return wav, int(sr)\n",
                "    if isinstance(audio, dict) and \"sampling_rate\" in audio and \"data\" in audio:\n",
                "        sr = int(audio[\"sampling_rate\"])\n        wav = _normalize_audio(audio[\"data\"])\n        return wav, sr\n",
                "    return None\n",
                "\n",
                "# --- Generation Functions ---\n",
                "\n",
                "def generate_voice_design(text, language, voice_description):\n",
                "    if not text or not text.strip():\n",
                "        return None, \"Error: Text is required.\"\n",
                "    if not voice_description or not voice_description.strip():\n",
                "        return None, \"Error: Voice description is required.\"\n",
                "    try:\n",
                "        tts = get_model(\"VoiceDesign\", \"1.7B\")\n",
                "        wavs, sr = tts.generate_voice_design(\n",
                "            text=text.strip(),\n",
                "            language=language,\n",
                "            instruct=voice_description.strip(),\n",
                "            non_streaming_mode=True,\n",
                "            max_new_tokens=2048,\n",
                "        )\n",
                "        return (sr, wavs[0]), \"Voice design generation completed successfully!\"\n",
                "    except Exception as e:\n",
                "        return None, f\"Error: {type(e).__name__}: {e}\"\n",
                "\n",
                "def generate_voice_clone(ref_audio, ref_text, target_text, language, use_xvector_only, model_size):\n",
                "    if not target_text or not target_text.strip():\n",
                "        return None, \"Error: Target text is required.\"\n",
                "    audio_tuple = _audio_to_tuple(ref_audio)\n",
                "    if audio_tuple is None:\n",
                "        return None, \"Error: Reference audio is required.\"\n",
                "    if not use_xvector_only and (not ref_text or not ref_text.strip()):\n",
                "        return None, \"Error: Reference text is required when 'Use x-vector only' is not enabled.\"\n",
                "    try:\n",
                "        tts = get_model(\"Base\", model_size)\n",
                "        wavs, sr = tts.generate_voice_clone(\n",
                "            text=target_text.strip(),\n",
                "            language=language,\n",
                "            ref_audio=audio_tuple,\n",
                "            ref_text=ref_text.strip() if ref_text else None,\n",
                "            x_vector_only_mode=use_xvector_only,\n",
                "            max_new_tokens=2048,\n",
                "        )\n",
                "        return (sr, wavs[0]), \"Voice clone generation completed successfully!\"\n",
                "    except Exception as e:\n",
                "        return None, f\"Error: {type(e).__name__}: {e}\"\n",
                "\n",
                "def generate_custom_voice(text, language, speaker, instruct, model_size):\n",
                "    if not text or not text.strip():\n",
                "        return None, \"Error: Text is required.\"\n",
                "    if not speaker:\n",
                "        return None, \"Error: Speaker is required.\"\n",
                "    try:\n",
                "        tts = get_model(\"CustomVoice\", model_size)\n",
                "        wavs, sr = tts.generate_custom_voice(\n",
                "            text=text.strip(),\n",
                "            language=language,\n",
                "            speaker=speaker.lower().replace(\" \", \"_\"),\n",
                "            instruct=instruct.strip() if instruct else None,\n",
                "            non_streaming_mode=True,\n",
                "            max_new_tokens=2048,\n",
                "        )\n",
                "        return (sr, wavs[0]), \"Generation completed successfully!\"\n",
                "    except Exception as e:\n",
                "        return None, f\"Error: {type(e).__name__}: {e}\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 4. Launch Gradio Interface\n",
                "\n",
                "SPEAKERS = [\n",
                "    \"Aiden\", \"Dylan\", \"Eric\", \"Ono_anna\", \"Ryan\", \"Serena\", \"Sohee\", \"Uncle_fu\", \"Vivian\"\n",
                "]\n",
                "LANGUAGES = [\"Auto\", \"Chinese\", \"English\", \"Japanese\", \"Korean\", \"French\", \"German\", \"Spanish\", \"Portuguese\", \"Russian\"]\n",
                "\n",
                "# Using the UI code from app.py but putting it inside a function to keep scope clean\n",
                "def build_ui():\n",
                "    theme = gr.themes.Soft(\n",
                "        font=[gr.themes.GoogleFont(\"Source Sans Pro\"), \"Arial\", \"sans-serif\"],\n",
                "    )\n",
                "    css = \"\"\"\n",
                "    .gradio-container {max-width: none !important;}\n",
                "    .tab-content {padding: 20px;}\n",
                "    \"\"\"\n",
                "    \n",
                "    with gr.Blocks(theme=theme, css=css, title=\"Qwen3-TTS Demo\") as demo:\n",
                "        gr.Markdown(\"# Qwen3-TTS Demo\")\n",
                "        \n",
                "        with gr.Tabs():\n",
                "            # Tab 1: Voice Design\n",
                "            with gr.Tab(\"Voice Design (Instruct to Speech)\"):\n",
                "                gr.Markdown(\"**Note: Only 1.7B model supports Voice Design.**\")\n",
                "                with gr.Row():\n",
                "                    with gr.Column(scale=1):\n",
                "                        vd_text = gr.Textbox(label=\"Text to Synthesize\", lines=3, placeholder=\"Enter text here...\")\n",
                "                        vd_language = gr.Dropdown(choices=LANGUAGES, value=\"Auto\", label=\"Language\")\n",
                "                        vd_desc = gr.Textbox(label=\"Voice Description\", lines=2, placeholder=\"e.g., A gentle, soothing female voice.\")\n",
                "                        vd_button = gr.Button(\"Generate\", variant=\"primary\")\n",
                "                    with gr.Column(scale=1):\n",
                "                        vd_output = gr.Audio(label=\"Generated Audio\")\n",
                "                        vd_status = gr.Markdown()\n",
                "                vd_button.click(\n",
                "                    generate_voice_design,\n",
                "                    inputs=[vd_text, vd_language, vd_desc],\n",
                "                    outputs=[vd_output, vd_status]\n",
                "                )\n",
                "\n",
                "            # Tab 2: Voice Clone (Zero-Shot)\n",
                "            with gr.Tab(\"Voice Clone (Zero-Shot)\"):\n",
                "                with gr.Row():\n",
                "                    with gr.Column(scale=1):\n",
                "                        vc_target_text = gr.Textbox(label=\"Target Text\", lines=3)\n",
                "                        vc_language = gr.Dropdown(choices=LANGUAGES, value=\"Auto\", label=\"Language\")\n",
                "                        with gr.Group():\n",
                "                            vc_ref_audio = gr.Audio(label=\"Reference Audio\", type=\"numpy\")\n",
                "                            vc_ref_text = gr.Textbox(label=\"Reference Text (Optional if 'Use x-vector only')\")\n",
                "                        vc_xvector = gr.Checkbox(label=\"Use x-vector only (ignores ref text)\", value=False)\n",
                "                        vc_model_size = gr.Radio(choices=MODEL_SIZES, value=\"1.7B\", label=\"Model Size\")\n",
                "                        vc_button = gr.Button(\"Generate\", variant=\"primary\")\n",
                "                    with gr.Column(scale=1):\n",
                "                        vc_output = gr.Audio(label=\"Generated Audio\")\n",
                "                        vc_status = gr.Markdown()\n",
                "                vc_button.click(\n",
                "                    generate_voice_clone,\n",
                "                    inputs=[vc_ref_audio, vc_ref_text, vc_target_text, vc_language, vc_xvector, vc_model_size],\n",
                "                    outputs=[vc_output, vc_status]\n",
                "                )\n",
                "\n",
                "            # Tab 3: Custom Voices (Pre-set)\n",
                "            with gr.Tab(\"Custom Voices (Pre-set)\"):\n",
                "                with gr.Row():\n",
                "                    with gr.Column(scale=1):\n",
                "                        cv_text = gr.Textbox(label=\"Text\", lines=3)\n",
                "                        cv_language = gr.Dropdown(choices=LANGUAGES, value=\"Auto\", label=\"Language\")\n",
                "                        cv_speaker = gr.Dropdown(choices=SPEAKERS, value=\"Serena\", label=\"Speaker\")\n",
                "                        cv_instruct = gr.Textbox(label=\"Instruction (Optional)\", placeholder=\"e.g., speaking fast and likely\")\n",
                "                        cv_model_size = gr.Radio(choices=MODEL_SIZES, value=\"1.7B\", label=\"Model Size\")\n",
                "                        cv_button = gr.Button(\"Generate\", variant=\"primary\")\n",
                "                    with gr.Column(scale=1):\n",
                "                        cv_output = gr.Audio(label=\"Generated Audio\")\n",
                "                        cv_status = gr.Markdown()\n",
                "                cv_button.click(\n",
                "                    generate_custom_voice,\n",
                "                    inputs=[cv_text, cv_language, cv_speaker, cv_instruct, cv_model_size],\n",
                "                    outputs=[cv_output, cv_status]\n",
                "                )\n",
                "\n",
                "    return demo\n",
                "\n",
                "demo = build_ui()\n",
                "demo.launch(share=True, debug=True)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}